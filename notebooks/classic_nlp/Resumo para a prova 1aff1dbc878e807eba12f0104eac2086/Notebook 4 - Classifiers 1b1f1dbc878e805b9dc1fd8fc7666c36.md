# Notebook 4 - Classifiers

# Bag-Of-Words

O modelo Bag of Words (BoW) √© uma t√©cnica de processamento de linguagem natural que transforma texto em representa√ß√µes num√©ricas. Ele funciona criando um "saco de palavras" onde:

- Cada palavra √∫nica do texto se torna uma caracter√≠stica (feature)
- A ordem das palavras √© ignorada
- A frequ√™ncia de cada palavra √© contada

Exemplo pr√°tico de classifica√ß√£o de livros:

Considere dois textos de sinopses:

> Com√©dia: "Uma hist√≥ria divertida e engra√ßada sobre uma fam√≠lia maluca em f√©rias"
Drama: "Uma narrativa profunda e emocionante sobre perda e supera√ß√£o"
> 

O BoW criaria um vocabul√°rio como:

```python
vocabulario = {
    'historia': 1, 'divertida': 2, 'engracada': 3, 'familia': 4, 
    'maluca': 5, 'ferias': 6, 'narrativa': 7, 'profunda': 8,
    'emocionante': 9, 'perda': 10, 'superacao': 11
}

```

Cada texto seria representado como um vetor:

```python
comedia = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]  # presen√ßa das palavras
drama =   [1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]

```

Um classificador poderia ent√£o **aprender padr√µes nestes vetores para categorizar novos textos em "com√©dia" ou "drama" baseado nas palavras presentes.**

<aside>
üí° Esta representa√ß√£o, embora simples, √© efetiva para muitas tarefas de classifica√ß√£o de texto, especialmente quando combinada com t√©cnicas de normaliza√ß√£o e sele√ß√£o de caracter√≠sticas.

</aside>

# Distribui√ß√£o de Bernoulli

A distribui√ß√£o de Bernoulli √© uma varia√ß√£o do modelo Bag-of-Words que considera apenas a presen√ßa (1) ou aus√™ncia (0) de palavras, ignorando suas frequ√™ncias.

Principais caracter√≠sticas:

- Cada palavra √© representada por um valor bin√°rio (0 ou 1)
- N√£o importa quantas vezes a palavra aparece no texto
- √ötil para textos curtos ou quando a frequ√™ncia n√£o √© relevante

Exemplo usando o caso anterior:

```python
# Mesmo com m√∫ltiplas ocorr√™ncias, s√≥ marcamos presen√ßa (1) ou aus√™ncia (0)
comedia = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]  # presen√ßa das palavras
drama =   [1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]

# Se "divertida" aparecesse 3 vezes, ainda seria representada como 1

```

<aside>
üí° A distribui√ß√£o de Bernoulli pode ser mais eficiente computacionalmente e menos sens√≠vel a outliers, j√° que n√£o precisamos lidar com contagens.

</aside>

# Regress√£o Log√≠stica

A Regress√£o Log√≠stica √© um algoritmo de classifica√ß√£o que estima a probabilidade de um evento pertencer a uma determinada classe.

Principais caracter√≠sticas:

- Modelo linear que usa uma fun√ß√£o sigm√≥ide para mapear valores para probabilidades entre 0 e 1
- Eficiente para problemas de classifica√ß√£o bin√°ria
- Fornece probabilidades interpret√°veis como output

Exemplo para classifica√ß√£o de texto:

```python
# Usando vetores BoW como input para regress√£o log√≠stica
from sklearn.linear_model import LogisticRegression

# X cont√©m os vetores BoW, y cont√©m as classes (0: drama, 1: com√©dia)
modelo = LogisticRegression()
modelo.fit(X, y)

# Obt√©m probabilidades para um novo texto
probabilidades = modelo.predict_proba(novo_texto_vetorizado) # Probabilidades
predict = modelo.predict(novo_texto_vetorizado) # Classifica√ß√£o
```

<aside>
üí° A Regress√£o Log√≠stica √© particularmente √∫til quando precisamos n√£o s√≥ da classifica√ß√£o, mas tamb√©m da probabilidade/confian√ßa da predi√ß√£o.

</aside>

Aqui est√° um resumo das caracter√≠sticas da Regress√£o Log√≠stica:

| **Caracter√≠stica** | **Descri√ß√£o** |
| --- | --- |
| Tipo de classifica√ß√£o | Bin√°ria (naturalmente) ou multiclasse (one-vs-rest) |
| Output | Probabilidades entre 0 e 1 |
| Complexidade | Baixa (modelo linear) |
| Interpretabilidade | Alta (coeficientes indicam import√¢ncia das features) |
| Requisitos de dados | Features num√©ricas, preferencialmente escaladas |
| Uso em NLP | Muito comum com BoW ou TF-IDF como input |

## Compara√ß√£o dos Modelos

Aqui est√° uma an√°lise comparativa detalhada dos tr√™s modelos discutidos:

| **Caracter√≠stica** | **Bag-of-Words** | **Distribui√ß√£o de Bernoulli** | **Regress√£o Log√≠stica** |
| --- | --- | --- | --- |
| Representa√ß√£o | Frequ√™ncia das palavras | Presen√ßa/aus√™ncia (bin√°rio) | Probabilidades (0-1) |
| Complexidade | M√©dia | Baixa | Baixa |
| Uso de mem√≥ria | Alto (armazena frequ√™ncias) | Baixo (apenas bin√°rio) | Baixo |
| Melhor cen√°rio | Textos longos com frequ√™ncias importantes | Textos curtos, documentos bin√°rios | Classifica√ß√£o bin√°ria com features num√©ricas |
| Limita√ß√µes | Ignora ordem das palavras | Perde informa√ß√£o de frequ√™ncia | Assume rela√ß√£o linear entre features |
| Interpretabilidade | M√©dia | Alta | Alta |
| Tipo de output | Vetores num√©ricos | Vetores bin√°rios | Probabilidades |

<aside>
üí° Cada modelo tem seus pontos fortes espec√≠ficos:
- BoW √© excelente para an√°lise detalhada de frequ√™ncia de palavras
- Bernoulli √© eficiente para classifica√ß√£o simples de textos curtos
- Regress√£o Log√≠stica oferece probabilidades interpret√°veis para tomada de decis√£o

</aside>

# TFIDF

- Term-Frequency-Inverse-Document-Frequency
    - TF: frequ√™ncia de cada termo em cada documento (d√° uma ideia da import√¢ncia de cada termo em cada documento)
    - DF: import√¢ncia de um termo para a cole√ß√£o inteira
        - Um termo com um DF pequeno, tende a ser mais relevante em um documento

## Resumo dos Conceitos

Neste notebook, foram abordados importantes conceitos de classifica√ß√£o de texto e processamento de linguagem natural:

- **Bag-of-Words (BoW):**
    - T√©cnica que transforma texto em representa√ß√£o num√©rica
    - Conta a frequ√™ncia de cada palavra no texto
    - Ignora a ordem das palavras
- **Distribui√ß√£o de Bernoulli:**
    - Varia√ß√£o do BoW que considera apenas presen√ßa (1) ou aus√™ncia (0)
    - Mais eficiente computacionalmente
    - Ideal para textos curtos
- **Regress√£o Log√≠stica:**
    - Algoritmo de classifica√ß√£o que estima probabilidades
    - Usa fun√ß√£o sigm√≥ide para mapear valores entre 0 e 1
    - Alta interpretabilidade e baixa complexidade
- **TFIDF (Term Frequency-Inverse Document Frequency):**
    - Combina frequ√™ncia do termo no documento (TF)
    - Considera a relev√¢ncia do termo na cole√ß√£o inteira (IDF)
    - Permite identificar termos mais distintivos em documentos

| **Modelo** | **Melhor Cen√°rio de Uso** | **Exemplo de Aplica√ß√£o** |
| --- | --- | --- |
| Bag-of-Words | - Textos longos
- Quando frequ√™ncia √© importante
- An√°lise de conte√∫do detalhada | - An√°lise de artigos cient√≠ficos
- Categoriza√ß√£o de documentos longos
- An√°lise de reviews detalhados |
| Distribui√ß√£o de Bernoulli | - Textos curtos
- Classifica√ß√£o simples
- Recursos computacionais limitados | - Classifica√ß√£o de tweets
- An√°lise de mensagens curtas
- Filtro de spam |
| Regress√£o Log√≠stica | - Necessidade de probabilidades
- Classifica√ß√£o bin√°ria
- Dados bem estruturados | - Previs√£o de sentimentos
- Detec√ß√£o de fraude
- Classifica√ß√£o de clientes |
| TFIDF | - Identifica√ß√£o de termos relevantes
- An√°lise de cole√ß√µes de documentos
- Busca por termos distintivos | - Sistemas de busca
- Recomenda√ß√£o de conte√∫do
- An√°lise de relev√¢ncia de documentos |