{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST 1: Leveraging LLMs for Feature Generation and Classification\n",
    "\n",
    "Typically, if our data has $N$ features, we need around $10N$ data items to reach peak performance with classic classifiers like Logistic Regression. Therefore, if our vocabulary has 10,000 words, we would need around 1 million items in the training set to reach peak performance.\n",
    "\n",
    "An interesting idea regarding this was explored in 2024 in [Balek, V., S'ykora, L., Sklen'ak, V., & Kliegr, T. (2024). LLM-based feature generation from text for interpretable machine learning. ArXiv, abs/2409.07132](https://arxiv.org/abs/2409.07132). The idea is to use an LLM to generate meaningful and interpretable features from text, and then use Logistic Regression for classification.\n",
    "\n",
    "For example, in the movie plots dataset, we could have features like:\n",
    "- \"Is the protagonist an animal?\" (0 or 1)\n",
    "- \"Does the plot indicate psychological suffering?\" (0 or 1)\n",
    "\n",
    "With a reasonable number of these features, our model could make predictions based on meaningful features instead of raw words.\n",
    "\n",
    "## Objectives\n",
    "* Perform feature extraction for a particular dataset\n",
    "* Compare performance and explainability of classifiers with different approaches. \n",
    "\n",
    "## Rules\n",
    "\n",
    "I highlight a few elements of our usual rules:\n",
    "\n",
    "* You are **NOT ALLOWED** to use AI to generate any code you are asked to make yourself. This includes ChatGPT, CoPilot and all similar generators.\n",
    "* You are **NOT ALLOWED** to use Google or any other search engine.\n",
    "* You are **ALLOWED** to use the offical documentations for libraries: \n",
    "    * [sklearn](https://scikit-learn.org/)\n",
    "    * [numpy](https://numpy.org/)\n",
    "    * [matplotlib](https://matplotlib.org/)\n",
    "    * [google AI studio](https://aistudio.google.com/)\n",
    "* You are **ALLOWED** to use previous code from this course as basis.\n",
    "* You **MUST** use the university's proctoring software to show you are complying with these rules\n",
    "* This task is **INDIVIDUAL**. DO NOT share your code or results with anyone else.\n",
    "\n",
    "## Tasks and Deliverables\n",
    "\n",
    "* At any point, refer to [Balek et al.](https://arxiv.org/abs/2409.07132). \n",
    "* Make a well-commented code to solve each one of the tasks below.\n",
    "* Each task will be evaluated as:\n",
    "    * Insufficient: task is not done, off-topic, or low-effort\n",
    "    * In process: task is incomplete, done with a clear conceptual error, or comments \n",
    "    * Proficient: everything works and comments are enough to understand what is being done\n",
    "    * Advanced: everything works, comments are enough to understand what is being done, and code is well organized and formated using functions, dataclasses, and other adequate structures.\n",
    "* This task should be finished by the end of the class.\n",
    "* After you are finished, submit the executed notebook in our LMS system.\n",
    "\n",
    "### 1. Dataset Preparation:\n",
    "Adapting Balek et al.'s strategy to our movie plot classification case, create a dataset with at least 100 labeled items and at least 5 meaningful features. None of the features can be the class itself (\"is this a drama plot?\"). Use a clear strategy to avoid exceeding free tier quotas. Store data locally in a format of your choice.\n",
    "\n",
    "### 2. Classification:\n",
    "Use the generated features to train a Logistic Regression model. Use cross-validation to select the best hyperparameters. Report accuracy and f1-score for your classifier.\n",
    "\n",
    "### 3. Performance Comparison\n",
    "Compare the performance of the following approaches:\n",
    "1. Traditional Bag-of-Words\n",
    "2. LLM-generated features with Logistic Regression\n",
    "3. Direct classification using LLM\n",
    "\n",
    "Use a bar plot to show the performance differences (choose either accuracy or F1-score).\n",
    "\n",
    "### 4. Improvement Strategies\n",
    "Determine whether labeling more items would improve system performance. Use data to justify your answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Plot</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The film is about a family who move to the sub...</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Before heading out to a baseball game at a nea...</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The plot is that of a black woman going to the...</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On a beautiful summer day a father and mother ...</td>\n",
       "      <td>drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A thug accosts a girl as she leaves her workpl...</td>\n",
       "      <td>drama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Plot   Genre\n",
       "0  The film is about a family who move to the sub...  comedy\n",
       "1  Before heading out to a baseball game at a nea...  comedy\n",
       "2  The plot is that of a black woman going to the...  comedy\n",
       "3  On a beautiful summer day a father and mother ...   drama\n",
       "4  A thug accosts a girl as she leaves her workpl...   drama"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/tiagoft/NLP/main/wiki_movie_plots_drama_comedy.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model construction - BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.50\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df_ = df.sample(100)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_['Plot'], df_['Genre'], test_size=0.2)\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "X_train_matrix = vectorizer.fit_transform(X_train)\n",
    "X_test_matrix = vectorizer.transform(X_test)\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(X_train_matrix, y_train)\n",
    "y_pred = model.predict(X_test_matrix)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.46\n",
      "Precision: 0.43\n",
      "F1: 0.40\n",
      "Balanced accuracy: 0.46\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, f1_score, balanced_accuracy_score\n",
    "\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'F1: {f1:.2f}')\n",
    "print(f'Balanced accuracy: {balanced_accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model construction - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.55\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "df_ = df.sample(100)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_['Plot'], df_['Genre'], test_size=0.2)\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "X_train_matrix = vectorizer.fit_transform(X_train)\n",
    "X_test_matrix = vectorizer.transform(X_test)\n",
    "\n",
    "model_lr = LogisticRegression()\n",
    "model_lr.fit(X_train_matrix, y_train)\n",
    "y_pred = model_lr.predict(X_test_matrix)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.43\n",
      "Precision: 0.44\n",
      "F1: 0.44\n",
      "Balanced accuracy: 0.43\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, f1_score, balanced_accuracy_score\n",
    "\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'F1: {f1:.2f}')\n",
    "print(f'Balanced accuracy: {balanced_accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
