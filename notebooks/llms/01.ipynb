{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edd29da6",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Suppose we are trying to search some information within the materials from this course. The course is reasonably well organized, and everything is within jupyter notebooks. Jupyter notebook files (.ipynb) are actually in JSON format.\n",
    "\n",
    "Choose any .ipynb file and open it using the json library in Python. Investigate:\n",
    "\n",
    "How can we find a cell within the notebook?\n",
    "How can we known if the cell contains Python code or Markdown annotations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b007a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['# Logistic Regression with Pytorch\\n',\n",
       "   '\\n',\n",
       "   \"Allright, everyone. Let's go ahead and start doing things. We will start with a brief review, so that we are all on the same page.\\n\",\n",
       "   '\\n',\n",
       "   '## Review: Logistic Regression with Scikit-Learn\\n',\n",
       "   '\\n',\n",
       "   'If you have reached this point, you probably know by heart how to define a Logistic Regression pipeline in Scikit Learn:\\n']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': 17,\n",
       "  'metadata': {},\n",
       "  'outputs': [],\n",
       "  'source': ['from sklearn.feature_extraction.text import CountVectorizer\\n',\n",
       "   'from sklearn.linear_model import LogisticRegression\\n',\n",
       "   'from sklearn.pipeline import Pipeline\\n',\n",
       "   '\\n',\n",
       "   'vectorizer = CountVectorizer(binary=True)\\n',\n",
       "   'classifier = LogisticRegression()\\n',\n",
       "   'pipeline = Pipeline([(\"vectorizer\", vectorizer),\\n',\n",
       "   '                        (\"classifier\", classifier)])']},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['Remember that when we call `pipeline.fit()`, we first fit the vectorizer and then, with the results, we fit the classifier. So, these two methods for training our model are absolutely equivalent:']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': 18,\n",
       "  'metadata': {},\n",
       "  'outputs': [],\n",
       "  'source': ['def train_the_pipeline(pipeline, X_train, y_train):\\n',\n",
       "   '    pipeline.fit(X_train, y_train)\\n',\n",
       "   '    return pipeline\\n',\n",
       "   '\\n',\n",
       "   'def train_each_part_separately(vectorizer, classifier, X_train, y_train):\\n',\n",
       "   '    X_train_vectorized = vectorizer.fit_transform(X_train)\\n',\n",
       "   '    classifier.fit(X_train_vectorized, y_train)\\n',\n",
       "   '    return vectorizer, classifier']},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['In fact, we could go ahead and have our model trained, so that the next steps in this lesson are going to be a bit smoother. We could fit our model to classify some texts with our usual dataset:']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': 19,\n",
       "  'metadata': {},\n",
       "  'outputs': [{'name': 'stdout', 'output_type': 'stream', 'text': ['0.71\\n']}],\n",
       "  'source': ['import pandas as pd\\n',\n",
       "   'from sklearn.model_selection import train_test_split\\n',\n",
       "   'from sklearn.metrics import accuracy_score\\n',\n",
       "   '\\n',\n",
       "   \"df = pd.read_csv('https://raw.githubusercontent.com/tiagoft/NLP/refs/heads/main/wiki_movie_plots_drama_comedy.csv').sample(1000)\\n\",\n",
       "   \"X_train, X_test, y_train, y_test = train_test_split(df['Plot'], df['Genre'], test_size=0.2, random_state=42)\\n\",\n",
       "   'vectorizer, classifier = train_each_part_separately(vectorizer, classifier, X_train, y_train)\\n',\n",
       "   'X_vect = vectorizer.transform(X_test)\\n',\n",
       "   'y_pred = classifier.predict(X_vect)\\n',\n",
       "   'print(accuracy_score(y_test, y_pred))']},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': [\"Now, let's recall how Logistic Regression works!\\n\",\n",
       "   '\\n',\n",
       "   'The fitted predictor expects an input with $d$ features.\\n',\n",
       "   '\\n',\n",
       "   'We use the `vectorizer` to map each text in `X_test` to a vector with $d$ elements. Each of these vectors is a line $[x_1, x_2, x_3 \\\\cdots x_d]$ in `X_vect` (note that the index $d$ in $x_d$ is the same as the number of features expected by the fitted predictor.\\n',\n",
       "   '\\n',\n",
       "   'Then, Logistic Regression uses its fitter weights $\\\\beta$ to calculate a weighted sum of the elements in the input, that is:\\n',\n",
       "   '\\n',\n",
       "   '$$\\n',\n",
       "   'z = \\\\beta_0 + x_1 \\\\beta_1 + x_2 \\\\beta_2 + \\\\cdots + x_d \\\\beta_d.\\n',\n",
       "   '$$\\n',\n",
       "   '\\n',\n",
       "   'Note that if we have a matrix made of $N$ lines of features (each line corresponding to a dataset item!), then we can calculate the output for each line $i$ using:\\n',\n",
       "   '\\n',\n",
       "   '$$\\n',\n",
       "   'z_i = \\\\beta_0 + x_{i,1} \\\\beta_1 + x_{i,2} \\\\beta_2 + \\\\cdots + x_{i,d} \\\\beta_d.\\n',\n",
       "   '$$\\n',\n",
       "   '\\n',\n",
       "   'This can be translated into a matrix multiplication:\\n',\n",
       "   '\\n',\n",
       "   '$$\\n',\n",
       "   '\\\\begin{bmatrix}\\n',\n",
       "   'z_1 \\\\\\\\\\n',\n",
       "   'z_2 \\\\\\\\\\n',\n",
       "   '\\\\cdots \\\\\\\\\\n',\n",
       "   'z_N \\\\end{bmatrix} = \\\\beta_0 + \\\\begin{bmatrix} \\n',\n",
       "   '                        x_{1,1} & x_{1,2} & \\\\cdots & x_{1,d} \\\\\\\\\\n',\n",
       "   '                        x_{2,1} & x_{2,2} & \\\\cdots & x_{2,d} \\\\\\\\\\n',\n",
       "   '                        \\\\cdots & \\\\cdots & \\\\cdots & \\\\cdots \\\\\\\\\\n',\n",
       "   '                        x_{N,1} & x_{N,2} & \\\\cdots & x_{N,d} \\\\\\\\\\n',\n",
       "   '                        \\\\end{bmatrix}\\n',\n",
       "   '                        \\\\begin{bmatrix}\\n',\n",
       "   '                        \\\\beta_1 \\\\\\\\\\n',\n",
       "   '                        \\\\beta_2 \\\\\\\\\\n',\n",
       "   '                        \\\\cdots \\\\\\\\\\n',\n",
       "   '                        \\\\beta_d\\n',\n",
       "   '                        \\\\end{bmatrix}.\\n',\n",
       "   '$$\\n',\n",
       "   '\\n',\n",
       "   'We can simplify this by defining a weight matrix $w = [\\\\beta_1, \\\\beta_2, \\\\cdots, \\\\beta_d]$, and an input matrix $X$ containing all elements $x_{i,j}$ so that:\\n',\n",
       "   '\\n',\n",
       "   '$$\\n',\n",
       "   'z = \\\\beta_0 + X w^T\\n',\n",
       "   '$$\\n',\n",
       "   '\\n',\n",
       "   'Last, we apply the logistic function to each element of the vector $z$, and then we have a prediction.\\n',\n",
       "   '\\n',\n",
       "   'The elements of $z$ are called *logits*, $\\\\beta_0$ is called *bias* and the elements of $w$ are called *weights*.\\n',\n",
       "   '\\n',\n",
       "   \"Good, but we didn't come this far to hear again about Logistic Regression. We now shall proceed!\"]},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['## Our first model with Pytorch\\n',\n",
       "   '\\n',\n",
       "   \"If you haven't done so, this is the moment to `pip install torch`. Pytorch is a framework that provides an API similar to Numpy with the addition of allowing operations in the GPU and easily using Autograd. Also, it contains many classes that are very useful for applications in Machine Learning. For example:\\n\"]},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': 20,\n",
       "  'metadata': {},\n",
       "  'outputs': [{'name': 'stdout',\n",
       "    'output_type': 'stream',\n",
       "    'text': ['Linear(in_features=3, out_features=1, bias=True)\\n']}],\n",
       "  'source': ['import torch.nn as nn\\n',\n",
       "   'linear_layer = nn.Linear(in_features=3, out_features=1)\\n',\n",
       "   'print(linear_layer)']},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['A linear layer takes something as an input $X$, multiplies it by a weight matrix $w$ and sums a bias $b$. In other words:\\n',\n",
       "   '\\n',\n",
       "   '$$\\n',\n",
       "   'z = \\\\beta_0 + X w^T\\n',\n",
       "   '$$\\n',\n",
       "   '\\n',\n",
       "   \"Sounds familiar? Let's see this operation working in practice:\\n\"]},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': 21,\n",
       "  'metadata': {},\n",
       "  'outputs': [{'name': 'stdout',\n",
       "    'output_type': 'stream',\n",
       "    'text': ['tensor([[-0.8340],\\n',\n",
       "     '        [-0.7707]], grad_fn=<AddmmBackward0>)\\n',\n",
       "     'Parameter containing:\\n',\n",
       "     'tensor([[ 0.4499, -0.2000, -0.2288]], requires_grad=True) Parameter containing:\\n',\n",
       "     'tensor([-0.1976], requires_grad=True)\\n',\n",
       "     'tensor([[-0.8340],\\n',\n",
       "     '        [-0.7707]], grad_fn=<AddBackward0>)\\n']}],\n",
       "  'source': ['import torch\\n',\n",
       "   'X = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\\n',\n",
       "   '\\n',\n",
       "   '# We can call the linear_layer, and it will perform its operation:\\n',\n",
       "   'output = linear_layer(X)\\n',\n",
       "   'print(output)\\n',\n",
       "   '\\n',\n",
       "   '# We can also access the weights and biases of the linear layer:\\n',\n",
       "   'w = linear_layer.weight\\n',\n",
       "   'b = linear_layer.bias\\n',\n",
       "   'print(w, b)\\n',\n",
       "   '\\n',\n",
       "   '# We can use these weights and biases to perform the operation manually:\\n',\n",
       "   'z = X @ w.T + b\\n',\n",
       "   'print(z)']},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['    \\n',\n",
       "   'Note that we defined the size of the weight matrix using the `in_features` and `out_features` in our linear layer. The number `in_features` is the dimension of the input (probably our $d$), and `out_features` allows us to calculate several $z_j$ vectors simultaneously and independently. For example, having `in_features=2` and `out_features=3` leads to:\\n',\n",
       "   '\\n',\n",
       "   '$$\\n',\n",
       "   '\\\\begin{bmatrix}\\n',\n",
       "   'z_{1,1} & z_{1,2} & z_{1,3} \\\\\\\\\\n',\n",
       "   'z_{2,1} & z_{2,2} & z_{2,3} \\\\\\\\\\n',\n",
       "   '\\\\cdots \\\\\\\\\\n',\n",
       "   'z_{N, 1} & z_{N,2} & z_{N,3} \\\\end{bmatrix} = \\\\beta_0 + \\\\begin{bmatrix} \\n',\n",
       "   '                        x_{1,1} & x_{1,2} & \\\\cdots & x_{1,d} \\\\\\\\\\n',\n",
       "   '                        x_{2,1} & x_{2,2} & \\\\cdots & x_{2,d} \\\\\\\\\\n',\n",
       "   '                        \\\\cdots & \\\\cdots & \\\\cdots & \\\\cdots \\\\\\\\\\n',\n",
       "   '                        x_{N,1} & x_{N,2} & \\\\cdots & x_{N,d} \\\\\\\\\\n',\n",
       "   '                        \\\\end{bmatrix}\\n',\n",
       "   '                        \\\\begin{bmatrix}\\n',\n",
       "   '                        \\\\beta_{1,1} & \\\\beta_{1,2} & \\\\beta{1,3}\\\\\\\\\\n',\n",
       "   '                        \\\\beta_{2,1} & \\\\beta_{2,2} & \\\\beta{2,3}\\\\\\\\\\n',\n",
       "   '                        \\\\cdots \\\\\\\\\\n',\n",
       "   '                        \\\\beta_{d,1} & \\\\beta_{d,2} & \\\\beta_{d,3}\\n',\n",
       "   '                        \\\\end{bmatrix}.\\n',\n",
       "   '$$\\n']},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['A linear layer takes something as an input $X$, multiplies it by a weight matrix $w$ and sums a bias $b$. In other words:\\n',\n",
       "   '\\n',\n",
       "   '$$\\n',\n",
       "   'z = \\\\beta_0 + X w^T\\n',\n",
       "   '$$\\n',\n",
       "   '\\n',\n",
       "   \"Sounds familiar? Let's see this operation working in practice:\"]},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': 22,\n",
       "  'metadata': {},\n",
       "  'outputs': [{'name': 'stdout',\n",
       "    'output_type': 'stream',\n",
       "    'text': ['tensor([[-0.8340],\\n',\n",
       "     '        [-0.7707]], grad_fn=<AddmmBackward0>)\\n',\n",
       "     'Parameter containing:\\n',\n",
       "     'tensor([[ 0.4499, -0.2000, -0.2288]], requires_grad=True) Parameter containing:\\n',\n",
       "     'tensor([-0.1976], requires_grad=True)\\n',\n",
       "     'tensor([[-0.8340],\\n',\n",
       "     '        [-0.7707]], grad_fn=<AddBackward0>)\\n']}],\n",
       "  'source': ['import torch\\n',\n",
       "   'X = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\\n',\n",
       "   '\\n',\n",
       "   '# We can call the linear_layer, and it will perform its operation:\\n',\n",
       "   'output = linear_layer(X)\\n',\n",
       "   'print(output)\\n',\n",
       "   '\\n',\n",
       "   '# We can also access the weights and biases of the linear layer:\\n',\n",
       "   'w = linear_layer.weight\\n',\n",
       "   'b = linear_layer.bias\\n',\n",
       "   'print(w, b)\\n',\n",
       "   '\\n',\n",
       "   '# We can use these weights and biases to perform the operation manually:\\n',\n",
       "   'z = X @ w.T + b\\n',\n",
       "   'print(z)']},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['    \\n',\n",
       "   'Note that we defined the size of the weight matrix using the `in_features` and `out_features` in our linear layer. The number `in_features` is the dimension of the input (probably our $d$), and `out_features` allows us to calculate several $z_j$ vectors simultaneously and independently. For example, having `in_features=2` and `out_features=3` leads to:\\n',\n",
       "   '\\n',\n",
       "   '$$\\n',\n",
       "   '\\\\begin{bmatrix}\\n',\n",
       "   'z_{1,1} & z_{1,2} & z_{1,3} \\\\\\\\\\n',\n",
       "   'z_{2,1} & z_{2,2} & z_{2,3} \\\\\\\\\\n',\n",
       "   '\\\\cdots \\\\\\\\\\n',\n",
       "   'z_{N, 1} & z_{N,2} & z_{N,3} \\\\end{bmatrix} = \\\\beta_0 + \\\\begin{bmatrix} \\n',\n",
       "   '                        x_{1,1} & x_{1,2} & \\\\cdots & x_{1,d} \\\\\\\\\\n',\n",
       "   '                        x_{2,1} & x_{2,2} & \\\\cdots & x_{2,d} \\\\\\\\\\n',\n",
       "   '                        \\\\cdots & \\\\cdots & \\\\cdots & \\\\cdots \\\\\\\\\\n',\n",
       "   '                        x_{N,1} & x_{N,2} & \\\\cdots & x_{N,d} \\\\\\\\\\n',\n",
       "   '                        \\\\end{bmatrix}\\n',\n",
       "   '                        \\\\begin{bmatrix}\\n',\n",
       "   '                        \\\\beta_{1,1} & \\\\beta_{1,2} & \\\\beta{1,3}\\\\\\\\\\n',\n",
       "   '                        \\\\beta_{2,1} & \\\\beta_{2,2} & \\\\beta{2,3}\\\\\\\\\\n',\n",
       "   '                        \\\\cdots \\\\\\\\\\n',\n",
       "   '                        \\\\beta_{d,1} & \\\\beta_{d,2} & \\\\beta_{d,3}\\n',\n",
       "   '                        \\\\end{bmatrix}.\\n',\n",
       "   '$$\\n',\n",
       "   '\\n',\n",
       "   'or, more compactly:\\n',\n",
       "   '\\n',\n",
       "   '$$\\n',\n",
       "   'z_{N \\\\times J} = x_{N \\\\times d} w^T_{d \\\\times j} + b_{1 \\\\times j}\\n',\n",
       "   '$$\\n',\n",
       "   '\\n',\n",
       "   'Note that $w \\\\in \\\\mathbb{R}^{j \\\\times d}$, hence $w^T \\\\in \\\\mathbb{R}^{d \\\\times j}$.\\n']},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['### Exercise 1\\n',\n",
       "   '\\n',\n",
       "   'In the formulation for the linear layer above, $z$ corresponds to an important part of a logistic regression (remember: logistic regression has an input, a decision function, and then a final probability estimate $P(C | x)$). Which of these parts corresponds to the linear layer operation, and what is missing to make a full logistic regression using the linear layer?']},\n",
       " {'cell_type': 'markdown', 'metadata': {}, 'source': []},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['## A linear layer is a linear predictor?\\n',\n",
       "   '\\n',\n",
       "   \"Allright, so the linear layer works essentially the same as the logistic regression. Let's show it!\\n\",\n",
       "   '\\n',\n",
       "   \"First, get back to the classifier we made in the beginning of this class. If you haven't fitted it yet, do it now.\\n\",\n",
       "   '\\n',\n",
       "   'In the code below, we get the weights from the fitted logistic regression and substitute them in our linear layer:\\n']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': 23,\n",
       "  'metadata': {},\n",
       "  'outputs': [],\n",
       "  'source': ['logistic_w = classifier.coef_\\n',\n",
       "   'logistic_b = classifier.intercept_\\n',\n",
       "   '\\n',\n",
       "   'w = torch.tensor(logistic_w, dtype=torch.float32)\\n',\n",
       "   'b = torch.tensor(logistic_b, dtype=torch.float32)\\n',\n",
       "   '\\n',\n",
       "   'linear_layer.weight.data = w\\n',\n",
       "   'linear_layer.bias.data = b']},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['Now, we can run the linear layer with `X_vect` and then apply a logistic function:']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': 24,\n",
       "  'metadata': {},\n",
       "  'outputs': [],\n",
       "  'source': ['X_vect_tensor = torch.tensor(X_vect.toarray(), dtype=torch.float32)\\n',\n",
       "   'output = linear_layer(X_vect_tensor)\\n',\n",
       "   'output_probs = torch.sigmoid(output)']},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['Note that our `output_probs` here is equal to the second column of the output of `classifier.predict_proba()`:']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': 25,\n",
       "  'metadata': {},\n",
       "  'outputs': [{'name': 'stdout',\n",
       "    'output_type': 'stream',\n",
       "    'text': ['[[0.00112143 0.99887857]\\n',\n",
       "     ' [0.52950462 0.47049538]\\n',\n",
       "     ' [0.01416174 0.98583826]\\n',\n",
       "     ' [0.07480901 0.92519099]\\n',\n",
       "     ' [0.65259003 0.34740997]]\\n',\n",
       "     'tensor([[0.9989],\\n',\n",
       "     '        [0.4705],\\n',\n",
       "     '        [0.9858],\\n',\n",
       "     '        [0.9252],\\n',\n",
       "     '        [0.3474]], grad_fn=<SliceBackward0>)\\n']}],\n",
       "  'source': ['y_probs = classifier.predict_proba(X_vect)\\n',\n",
       "   'print(y_probs[:5])\\n',\n",
       "   'print(output_probs[:5])']},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['In fact, we could simply decide our class by thresholding our output:']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': 26,\n",
       "  'metadata': {},\n",
       "  'outputs': [{'name': 'stdout', 'output_type': 'stream', 'text': ['0.71\\n']}],\n",
       "  'source': ['binary_out = (output_probs > 0.5).numpy().astype(int)\\n',\n",
       "   'binary_y = (y_test==classifier.classes_[1]).astype(int)\\n',\n",
       "   'print(accuracy_score(binary_y, binary_out))']},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['## How to train a Logistic Regression\\n',\n",
       "   '\\n',\n",
       "   'Remember that our classifier outputs:\\n',\n",
       "   '\\n',\n",
       "   '\\n',\n",
       "   '$$\\n',\n",
       "   '\\\\hat{y} = \\\\sigma(z) = \\\\sigma(Xw^T + b),\\n',\n",
       "   '$$\\n',\n",
       "   'where $\\\\sigma()$ denotes the sigmoid (or logistic) function?\\n',\n",
       "   '\\n',\n",
       "   'The procedure to adjust the weights and biases in our linear layer is to use actual examples of outputs ($y$) and compare them to our estimate $\\\\hat{y}$. They are probably not going to be the same, so we can calculate how $y$ and $\\\\hat{y}$ are different using a loss function $L(y, \\\\hat{y})$. Then, we are going to calculate the derivative of $L$ with respect to all weights and biases, that is, we will have:\\n',\n",
       "   '\\n',\n",
       "   '$$\\n',\n",
       "   'g_i = \\\\frac{d L}{d p_i}\\n',\n",
       "   '$$\\n',\n",
       "   '\\n',\n",
       "   'for each parameter $p_i$ in the linear layer (either a weight or a bias). The parameters in the linear layer are the weights and biases. \\n',\n",
       "   '\\n',\n",
       "   'You might have noticed that $g$ is a vector of derivatives. Actually, you might even remember that it is called a *gradient* vector written as $\\\\nabla L$.\\n',\n",
       "   '\\n',\n",
       "   \"Remember that the gradient points towards the direction in which $L$ grows the most if we change the parameters $p$? So, let's do the opposite! We want to decrease our loss, so we will change our parameters by *subtracting* a little bit of the gradient vector, that is, we iteractively update the parameters as:\\n\",\n",
       "   '\\n',\n",
       "   '$$\\n',\n",
       "   'p_i \\\\leftarrow \\\\alpha \\\\frac{\\\\partial L}{\\\\partial p_i},\\n',\n",
       "   '$$\\n',\n",
       "   '\\n',\n",
       "   'where $\\\\alpha$ is a small value of our choice. A lower $\\\\alpha$ means steps will be smaller, which can make it take longer to converge, whereas a higher value can lead to instabilities.\\n',\n",
       "   '\\n',\n",
       "   'We could be fancier and write this in vector form:\\n',\n",
       "   '\\n',\n",
       "   '$$\\n',\n",
       "   'p \\\\leftarrow \\\\alpha \\\\nabla_{\\\\mathbf{p}} L.\\n',\n",
       "   '$$\\n',\n",
       "   '\\n',\n",
       "   'What we usually do is to calculate the gradient $g$ for each item in the training data, and then sum (or average?) it before applying the step to the parameters. This process of going through the whole dataset is called *epoch*.\\n',\n",
       "   '\\n',\n",
       "   \"Now, how do we implement this? Let's go!\\n\"]},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': 27,\n",
       "  'metadata': {},\n",
       "  'outputs': [],\n",
       "  'source': ['# First, we prepare our data:\\n',\n",
       "   'X_vect_train = torch.tensor(vectorizer.transform(X_train).toarray(), dtype=torch.float32)\\n',\n",
       "   'y_train_ = (y_train == classifier.classes_[1]).astype(int).values\\n',\n",
       "   'y_train_vect = torch.tensor( y_train_, dtype=torch.float32).reshape( -1, 1)']},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['    \\n',\n",
       "   'Ok, now we are going to get into an optimization loop. When using Pytorch, we must first define an optimizer - we will use SGD, which is literally the gradient descent algorithm we have seen so far. Then, we will go into a training loop consisting of:\\n',\n",
       "   '\\n',\n",
       "   '1. Zeroing the gradient in the optimizer to reset its state,\\n',\n",
       "   '1. Calculate the output of the classifier\\n',\n",
       "   '1. Claculate the loss related to the output\\n',\n",
       "   '1. Calculate (going back into the layer!) the gradient of the loss using [autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)\\n',\n",
       "   '1. Apply the gradient to our parameters\\n',\n",
       "   '\\n',\n",
       "   'It goes like this:\\n']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': 28,\n",
       "  'metadata': {},\n",
       "  'outputs': [{'name': 'stdout',\n",
       "    'output_type': 'stream',\n",
       "    'text': ['Entering loop\\n',\n",
       "     '201.01898193359375\\n',\n",
       "     '197.97332763671875\\n',\n",
       "     '195.9878387451172\\n',\n",
       "     '194.46324157714844\\n',\n",
       "     '193.14129638671875\\n']}],\n",
       "  'source': ['\\n',\n",
       "   \"# Let's start with a new linear layer:\\n\",\n",
       "   'clf = nn.Linear(in_features=len(vectorizer.vocabulary_), out_features=1)\\n',\n",
       "   '\\n',\n",
       "   '# We will also define an optimizer:\\n',\n",
       "   'optimizer = torch.optim.SGD(clf.parameters(), lr=1e-4) # lr is the learning rate - this is our alpha\\n',\n",
       "   '\\n',\n",
       "   'print(\"Entering loop\")\\n',\n",
       "   '# And now, this is the training loop:\\n',\n",
       "   'for epoch in range(5):\\n',\n",
       "   '    optimizer.zero_grad()\\n',\n",
       "   '    output = clf(X_vect_train)\\n',\n",
       "   '    output_probs = torch.sigmoid(output)\\n',\n",
       "   '    loss = torch.sum( (output_probs-y_train_vect)**2 )\\n',\n",
       "   '    loss.backward()\\n',\n",
       "   '    optimizer.step()\\n',\n",
       "   '    print(loss.item())\\n']},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['How does pytorch know that gradients should be calculated? Well, they are calculated by default. That is why, in inference (or: in testing), we use `torch.no_grad()` and `model.eval()` - this saves memory, and saves computation time as well:']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': 29,\n",
       "  'metadata': {},\n",
       "  'outputs': [{'name': 'stdout',\n",
       "    'output_type': 'stream',\n",
       "    'text': ['0.585\\n']}],\n",
       "  'source': ['X_vect_test = torch.tensor(vectorizer.transform(X_test).toarray(), dtype=torch.float32)\\n',\n",
       "   'y_test_ = (y_test == classifier.classes_[1]).astype(int).values\\n',\n",
       "   'with torch.no_grad():\\n',\n",
       "   '    clf.eval()\\n',\n",
       "   '    y_pred = (torch.sigmoid(clf(X_vect_test)) > 0.5).numpy().astype(int)\\n',\n",
       "   'print(accuracy_score(y_test_, y_pred))']},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['## Watching our training process\\n',\n",
       "   '\\n',\n",
       "   \"If you simply increased the number of epochs to 1000 in our loop, you probably had an overflow of `print` statements with the current loss. We probably don't want to see that - rather, we want a figure!\\n\",\n",
       "   '\\n',\n",
       "   'Up until a few years ago, we had to make this figure all by ourselves, with a procedure such as:\\n']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': 32,\n",
       "  'metadata': {},\n",
       "  'outputs': [{'name': 'stderr',\n",
       "    'output_type': 'stream',\n",
       "    'text': ['100%|██████████| 100/100 [00:00<00:00, 182.95it/s]\\n']}],\n",
       "  'source': ['from tqdm import tqdm # This will make us a progress bar\\n',\n",
       "   '\\n',\n",
       "   'clf = nn.Linear(in_features=len(vectorizer.vocabulary_), out_features=1)\\n',\n",
       "   'optimizer = torch.optim.SGD(clf.parameters(), lr=1e-4) # lr is the learning rate - this is our alpha\\n',\n",
       "   '\\n',\n",
       "   '# And now, a loop that is equal for everyone:\\n',\n",
       "   'losses = []\\n',\n",
       "   'for epoch in tqdm(range(100)):\\n',\n",
       "   '    optimizer.zero_grad()\\n',\n",
       "   '    output = clf(X_vect_train)\\n',\n",
       "   '    output_probs = torch.sigmoid(output)\\n',\n",
       "   '    loss = torch.sum( (output_probs-y_train_vect)**2 )\\n',\n",
       "   '    loss.backward()\\n',\n",
       "   '    optimizer.step()\\n',\n",
       "   '    losses.append(loss.item())\\n']},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': 34,\n",
       "  'metadata': {},\n",
       "  'outputs': [{'data': {'image/png': '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',\n",
       "     'text/plain': ['<Figure size 300x200 with 1 Axes>']},\n",
       "    'metadata': {},\n",
       "    'output_type': 'display_data'}],\n",
       "  'source': ['import matplotlib.pyplot as plt\\n',\n",
       "   'plt.figure(figsize=(3,2))\\n',\n",
       "   'plt.plot(losses)\\n',\n",
       "   \"plt.xlabel('Epoch')\\n\",\n",
       "   \"plt.ylabel('Loss')\\n\",\n",
       "   'plt.show()\\n']},\n",
       " {'cell_type': 'markdown',\n",
       "  'metadata': {},\n",
       "  'source': ['\\n',\n",
       "   'Nowadays, we have plenty of frameworks to store this same data: there is Weights and Biases, MLFlow, TensorBoard, and so on. Please, do feel free to use any of them. We will not adopt any of them for this course because we might get trapped within very specific details of them.\\n',\n",
       "   '\\n',\n",
       "   '\\n',\n",
       "   '**The main lessons here are:**\\n',\n",
       "   '\\n',\n",
       "   '* Save your data to a variable, and plot figures later (do NOT plot while doing training!)\\n',\n",
       "   '* Do not use the terminal to debug your loss\\n',\n",
       "   '* Use `tqdm` so we know your code is running.\\n',\n",
       "   '\\n',\n",
       "   '## Exercise 2\\n',\n",
       "   '\\n',\n",
       "   'Manipulate the code above to find the following:\\n',\n",
       "   '\\n',\n",
       "   '1. When we use `torch.sum( (output_probs-y_train_vect)**2 )` to calculate the loss, we are essentially saying that larger datasets have larger losses. Change this calculation so the loss is independent of the number of items in the dataset.\\n',\n",
       "   '1. You might want to increase the learning rate `lr` to make your training faster. What happens if you increase it too much? Can you guess why?\\n',\n",
       "   '1. Change your code to show that the `accuracy` (at least in the training set) tends to decrease together with the `loss`.\\n',\n",
       "   '1. How many epochs do you actually need in this training process?']},\n",
       " {'cell_type': 'markdown', 'metadata': {}, 'source': []}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_notebook_cells(path: str) -> list:\n",
    "    \"\"\"\n",
    "    Carrega um arquivo .ipynb e retorna a lista de células.\n",
    "    \"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        nb = json.load(f)\n",
    "    return nb.get('cells', [])\n",
    "\n",
    "load_notebook_cells('../deep_learning/01-from_sklearn_to_pytorch.ipynb')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4e5d8e",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "The simplest way to search for text is using keywords. Improve your code so that it:\n",
    "\n",
    "1. Collects a keyword from the user, and\n",
    "1. Indicates all notebooks/cells that contain that keyword.\n",
    "\n",
    "Reflect: what is the best way to present these results to the user?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2a869ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('../deep_learning\\\\01-from_sklearn_to_pytorch.ipynb',\n",
       "  1,\n",
       "  'code',\n",
       "  'from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline  vectorizer = CountVectorizer(binary=True) classif'),\n",
       " ('../deep_learning\\\\01-from_sklearn_to_pytorch.ipynb',\n",
       "  5,\n",
       "  'code',\n",
       "  \"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score  df = pd.read_csv('https://raw.githubusercontent.com/tiagoft/NLP/refs/heads/main/wik\"),\n",
       " ('../deep_learning\\\\01-from_sklearn_to_pytorch.ipynb',\n",
       "  8,\n",
       "  'code',\n",
       "  'import torch.nn as nn linear_layer = nn.Linear(in_features=3, out_features=1) print(linear_layer)'),\n",
       " ('../deep_learning\\\\01-from_sklearn_to_pytorch.ipynb',\n",
       "  10,\n",
       "  'code',\n",
       "  'import torch X = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])  # We can call the linear_layer, and it will perform its operation: output = linear_layer(X) print(output)  # We can also access the w'),\n",
       " ('../deep_learning\\\\01-from_sklearn_to_pytorch.ipynb',\n",
       "  13,\n",
       "  'code',\n",
       "  'import torch X = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])  # We can call the linear_layer, and it will perform its operation: output = linear_layer(X) print(output)  # We can also access the w'),\n",
       " ('../deep_learning\\\\01-from_sklearn_to_pytorch.ipynb',\n",
       "  15,\n",
       "  'markdown',\n",
       "  '### Exercise 1  In the formulation for the linear layer above, $z$ corresponds to an important part of a logistic regression (remember: logistic regression has an input, a decision function, and then '),\n",
       " ('../deep_learning\\\\01-from_sklearn_to_pytorch.ipynb',\n",
       "  32,\n",
       "  'code',\n",
       "  'from tqdm import tqdm # This will make us a progress bar  clf = nn.Linear(in_features=len(vectorizer.vocabulary_), out_features=1) optimizer = torch.optim.SGD(clf.parameters(), lr=1e-4) # lr is the le'),\n",
       " ('../deep_learning\\\\01-from_sklearn_to_pytorch.ipynb',\n",
       "  33,\n",
       "  'code',\n",
       "  \"import matplotlib.pyplot as plt plt.figure(figsize=(3,2)) plt.plot(losses) plt.xlabel('Epoch') plt.ylabel('Loss') plt.show() \"),\n",
       " ('../deep_learning\\\\02-end_to_end_neural_networks.ipynb',\n",
       "  0,\n",
       "  'markdown',\n",
       "  '# End-to-end Neural Networks for NLP  ## Word Embeddings  If everything is going well, you have used PyTorch to make a classifier based on logistic regression. Also, you are probably mildly annoyed by'),\n",
       " ('../deep_learning\\\\02-end_to_end_neural_networks.ipynb',\n",
       "  1,\n",
       "  'code',\n",
       "  'import torch import torch.nn as nn  # Parameters of the embedding layer vocab_size = 100  # Number of unique tokens embedding_dim = 2  # You can choose the dimension of the embeddings embedding_layer '),\n",
       " ('../deep_learning\\\\02-end_to_end_neural_networks.ipynb',\n",
       "  4,\n",
       "  'code',\n",
       "  'import sentencepiece as spm from io import StringIO  # Your input data as a string input_data = \"\"\"Was ever feather so lightly blown to and fro as this multitude? The name of Henry the Fifth hales the'),\n",
       " ('../deep_learning\\\\02-end_to_end_neural_networks.ipynb',\n",
       "  14,\n",
       "  'code',\n",
       "  'import torch.nn.functional as F  class SimpleClassifier(nn.Module):     def __init__(self, vocab_size, embedding_dim):         super().__init__()         self.embedding = nn.Embedding(             num'),\n",
       " ('../deep_learning\\\\02-end_to_end_neural_networks.ipynb',\n",
       "  15,\n",
       "  'code',\n",
       "  \"import pandas as pd  from sklearn.model_selection import train_test_split  df = pd.read_csv(     'https://raw.githubusercontent.com/tiagoft/NLP/main/wiki_movie_plots_drama_comedy.csv' ) # Split the da\"),\n",
       " ('../deep_learning\\\\02-end_to_end_neural_networks.ipynb',\n",
       "  16,\n",
       "  'code',\n",
       "  'from tqdm import tqdm   # We will also define an optimizer: optimizer = torch.optim.SGD(model.parameters(), lr=1e0) # lr is the learning rate - this is our alpha  print(\"Entering loop\") # And now, thi'),\n",
       " ('../deep_learning\\\\02-end_to_end_neural_networks.ipynb',\n",
       "  17,\n",
       "  'code',\n",
       "  \"import matplotlib.pyplot as plt  plt.figure(figsize=(5,3)) plt.plot(losses) plt.xlabel('Epoch') plt.ylabel('Loss') plt.title('Training Loss over Epochs') plt.show()\"),\n",
       " ('../deep_learning\\\\02-end_to_end_neural_networks.ipynb',\n",
       "  19,\n",
       "  'code',\n",
       "  'from sklearn.metrics import accuracy_score, f1_score, classification_report  # Get the predictions for the test set with torch.no_grad():     model.eval()     tokens = sp.encode_as_ids(X_test)     tok'),\n",
       " ('../deep_learning\\\\02-end_to_end_neural_networks.ipynb',\n",
       "  20,\n",
       "  'markdown',\n",
       "  '## Some steps on optimization  You may have noticed that we are using the MSE loss in the code above. This is definitely not optimal, because reductions in MSE+ does not necessarily correspond to incr'),\n",
       " ('../deep_learning\\\\02-end_to_end_neural_networks.ipynb',\n",
       "  21,\n",
       "  'code',\n",
       "  'from tqdm import tqdm  # We will also define an optimizer: optimizer = torch.optim.Adam(     model.parameters(), lr=1e0)  # lr is the learning rate - this is our alpha  print(\"Entering loop\") # And no'),\n",
       " ('../deep_learning\\\\02-end_to_end_neural_networks.ipynb',\n",
       "  22,\n",
       "  'code',\n",
       "  \"import matplotlib.pyplot as plt  plt.figure(figsize=(5,3)) plt.plot(losses) plt.xlabel('Epoch') plt.ylabel('Loss') plt.title('Training Loss over Epochs') plt.show()\"),\n",
       " ('../deep_learning\\\\02-end_to_end_neural_networks.ipynb',\n",
       "  23,\n",
       "  'code',\n",
       "  'from sklearn.metrics import accuracy_score, f1_score, classification_report  # Get the predictions for the test set with torch.no_grad():     model.eval()     tokens = sp.encode_as_ids(X_test)     tok'),\n",
       " ('../deep_learning\\\\03-sequence_models.ipynb',\n",
       "  11,\n",
       "  'code',\n",
       "  'import torch.nn.functional as F import torch import torch.nn as nn  vocab_size = 5000 padding_idx = 3 hidden_dim = 20 # this is dh embedding_dim = 30 # this is d  class ClassifierWithRNN(nn.Module):  '),\n",
       " ('../deep_learning\\\\03-sequence_models.ipynb',\n",
       "  13,\n",
       "  'code',\n",
       "  'import kagglehub import os  print(\"FakeNewsNet\") fnnpath = kagglehub.dataset_download(\"algord/fake-news\") print(\"Path to dataset files:\", fnnpath) files = os.listdir(fnnpath) print(\"Files in dataset p'),\n",
       " ('../deep_learning\\\\03-sequence_models.ipynb',\n",
       "  14,\n",
       "  'code',\n",
       "  \"# Start with this pre-trained tokenizer, so you dont have to train your own: import sentencepiece as spm sp = spm.SentencePieceProcessor() sp.load('fakenews_tokenizer.model') padding_idx = sp.piece_to\"),\n",
       " ('../deep_learning\\\\03-sequence_models.ipynb',\n",
       "  16,\n",
       "  'code',\n",
       "  '# This is my solution. DO NOT LOOK AT IT before trying yours.                 class ClassifierWithRNN(nn.Module):     def __init__(self, vocab_size, hidden_dim, embedding_dim):         super().__init_'),\n",
       " ('../deep_learning\\\\03-sequence_models.ipynb',\n",
       "  18,\n",
       "  'code',\n",
       "  'import matplotlib.pyplot as plt plt.figure(figsize=(3,3)) plt.plot(losses) plt.show()'),\n",
       " ('../deep_learning\\\\03-sequence_models.ipynb',\n",
       "  20,\n",
       "  'code',\n",
       "  'import matplotlib.pyplot as plt  e1 = embeddings_pre.cpu().detach().numpy() #y = y.cpu().detach().numpy() plt.figure(figsize=(6, 3)) plt.subplot(1,2,1) plt.scatter(e1[:,0], e1[:,1], c=y, alpha=0.2) pl'),\n",
       " ('../deep_learning\\\\04-multiclass_classifiers_and_language_models.ipynb',\n",
       "  1,\n",
       "  'code',\n",
       "  \"from sklearn.datasets import fetch_20newsgroups newsgroups_train = fetch_20newsgroups(subset='train') X = newsgroups_train.data y = newsgroups_train.target print(X[0:5]) print(y[0:5]) print(len(set(y)\"),\n",
       " ('../deep_learning\\\\04-multiclass_classifiers_and_language_models.ipynb',\n",
       "  2,\n",
       "  'markdown',\n",
       "  '## Multiclass Logistic Regression  Although sklearn deals with multiclass classification automatically, in PyTorch we have to take care of some things:  1. The number of outputs of the classification '),\n",
       " ('../deep_learning\\\\04-multiclass_classifiers_and_language_models.ipynb',\n",
       "  3,\n",
       "  'code',\n",
       "  \"import sentencepiece as spm sp = spm.SentencePieceProcessor() sp.load('fakenews_tokenizer.model') padding_idx = sp.piece_to_id('<PAD>')  def pad_to_len(sequences, pad_idx, max_len):     padded = []   \"),\n",
       " ('../deep_learning\\\\04-multiclass_classifiers_and_language_models.ipynb',\n",
       "  4,\n",
       "  'code',\n",
       "  'import pandas as pd from pathlib import Path from tqdm import tqdm import torch import torch.nn as nn import torch.nn.functional as F  class MulticlassClassifier(nn.Module):      def __init__(self, vo'),\n",
       " ('../deep_learning\\\\04-multiclass_classifiers_and_language_models.ipynb',\n",
       "  5,\n",
       "  'code',\n",
       "  \"from sklearn.datasets import fetch_20newsgroups newsgroups_train = fetch_20newsgroups(subset='test') X = newsgroups_train.data y = newsgroups_train.target  allowed_classes = set(range(20)) n_classes =\"),\n",
       " ('../deep_learning\\\\04-multiclass_classifiers_and_language_models.ipynb',\n",
       "  7,\n",
       "  'code',\n",
       "  'from sklearn.metrics import accuracy_score y_pred_probs = model(tokens) y_pred = torch.argmax(y_pred_probs, axis=1) y_pred = y_pred.detach().cpu().numpy() y_true = y.detach().cpu().numpy()  accuracy ='),\n",
       " ('../deep_learning\\\\04-multiclass_classifiers_and_language_models.ipynb',\n",
       "  9,\n",
       "  'code',\n",
       "  'import matplotlib.pyplot as plt from sklearn.decomposition import PCA from sklearn.manifold import TSNE  indices = torch.randperm(tokens.size(0))[:1000] tokens_sample = tokens[indices] y_sample = y[in'),\n",
       " ('../deep_learning\\\\04-multiclass_classifiers_and_language_models.ipynb',\n",
       "  11,\n",
       "  'code',\n",
       "  'import torch from torch.utils.data import Dataset, DataLoader  class MyDataset(Dataset):     def __init__(self, data, labels):         self.data = data # If data does not fit memory, change this to e.'),\n",
       " ('../deep_learning\\\\04-multiclass_classifiers_and_language_models.ipynb',\n",
       "  17,\n",
       "  'code',\n",
       "  \"# This is my solution. Don't look at it!    import torch import torch.nn as nn import torch.nn.functional as F  import sentencepiece as spm sp = spm.SentencePieceProcessor() sp.load('fakenews_tokenize\"),\n",
       " ('../deep_learning\\\\04-multiclass_classifiers_and_language_models.ipynb',\n",
       "  18,\n",
       "  'code',\n",
       "  'import matplotlib.pyplot as plt from sklearn.decomposition import PCA from sklearn.manifold import TSNE  indices = torch.randperm(tokens.size(0))[:1000] tokens_sample = tokens[indices] y_sample = y[in'),\n",
       " ('../deep_learning\\\\05-pretrained_bert.ipynb',\n",
       "  5,\n",
       "  'code',\n",
       "  'from transformers import pipeline unmasker = pipeline(\\'fill-mask\\', model=\\'bert-base-uncased\\') unmasker(\"Remove some parts [MASK] a sentence.\")'),\n",
       " ('../deep_learning\\\\05-pretrained_bert.ipynb',\n",
       "  12,\n",
       "  'code',\n",
       "  'from transformers import BertTokenizer, BertModel tokenizer = BertTokenizer.from_pretrained(\\'bert-base-uncased\\') model = BertModel.from_pretrained(\"bert-base-uncased\") text = \"Replace me by any text y'),\n",
       " ('../deep_learning\\\\05-pretrained_bert.ipynb',\n",
       "  17,\n",
       "  'code',\n",
       "  'import pandas as pd from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.model_selection import train_test_split from sklearn.metrics import classific'),\n",
       " ('../deep_learning\\\\05-pretrained_bert.ipynb',\n",
       "  20,\n",
       "  'code',\n",
       "  \"# This is my solution - do not copy it!  # Step 0: get data import pandas as pd df = pd.read_csv('https://raw.githubusercontent.com/tiagoft/NLP/main/wiki_movie_plots_drama_comedy.csv').sample(1000) X \"),\n",
       " ('../deep_learning\\\\05-pretrained_bert.ipynb',\n",
       "  21,\n",
       "  'code',\n",
       "  \"embeddings = np.load('bert_embeddings.npy') from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(embeddings, y, test_size=0.2, random_state=42) from\"),\n",
       " ('../deep_learning\\\\06-practice_in_classification.ipynb',\n",
       "  2,\n",
       "  'code',\n",
       "  'import kagglehub import os import pandas as pd from pathlib import Path  path = kagglehub.dataset_download(\"hgultekin/bbcnewsarchive\") print(os.listdir(path)) df = pd.read_csv(Path(path) / \"bbc-news-d'),\n",
       " ('../deep_learning\\\\06-practice_in_classification.ipynb',\n",
       "  5,\n",
       "  'code',\n",
       "  'from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeli'),\n",
       " ('../deep_learning\\\\06-practice_in_classification.ipynb',\n",
       "  12,\n",
       "  'code',\n",
       "  '#ex2  from transformers import BertTokenizer, BertModel from tqdm import tqdm import numpy as np  tokenizer = BertTokenizer.from_pretrained(\\'bert-base-uncased\\') model = BertModel.from_pretrained(\"bert'),\n",
       " ('../deep_learning\\\\06-practice_in_classification.ipynb',\n",
       "  13,\n",
       "  'code',\n",
       "  \"embeddings = np.load('bert_embeddings.npy') from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import classification_report, \"),\n",
       " ('../deep_learning\\\\06-practice_in_classification.ipynb',\n",
       "  14,\n",
       "  'code',\n",
       "  \"# confusion matrix from sklearn.metrics import confusion_matrix,  ConfusionMatrixDisplay  c = confusion_matrix(y_test, y_pred, normalize='true') print(c) _ = ConfusionMatrixDisplay(c, display_labels=c\"),\n",
       " ('../deep_learning\\\\06-practice_in_classification.ipynb',\n",
       "  15,\n",
       "  'code',\n",
       "  \"import matplotlib.pyplot as plt  plt.figure(figsize=(10, 6)) plt.bar(['Bag-of-Words', 'BERT'], [acc_bow, acc_bert], color=['blue', 'orange']) plt.ylabel('Accuracy') plt.title('Comparison of Bag-of-Wor\"),\n",
       " ('../deep_learning\\\\06-practice_in_classification.ipynb',\n",
       "  16,\n",
       "  'code',\n",
       "  'from sklearn.decomposition import PCA from sklearn.manifold import TSNE  pca = PCA(n_components=2) embeddings_pos_pca = pca.fit_transform(embeddings) tsne = TSNE(n_components=2, perplexity=5) embeddin'),\n",
       " ('../deep_learning\\\\06-practice_in_classification.ipynb',\n",
       "  17,\n",
       "  'code',\n",
       "  \"import matplotlib.pyplot as plt import numpy as np  cats = df['category'].astype('category') codes = cats.cat.codes.values labels = cats.cat.categories  plt.figure(figsize=(15, 10))  # PCA ax1 = plt.s\"),\n",
       " ('../deep_learning\\\\07-lost_in_translation.ipynb',\n",
       "  1,\n",
       "  'code',\n",
       "  'import os  os.environ[\"ARGOS_DEVICE_TYPE\"] = \"cuda\" # or \"cpu\"  import argostranslate.package import argostranslate.translate  # First, we need to configure the translator: def _configure_argos(from_c'),\n",
       " ('../deep_learning\\\\07-lost_in_translation.ipynb',\n",
       "  4,\n",
       "  'code',\n",
       "  \"from transformers import pipeline unmasker = pipeline('fill-mask', model='bert-base-multilingual-uncased') \"),\n",
       " ('../deep_learning\\\\08-mlp.ipynb',\n",
       "  1,\n",
       "  'code',\n",
       "  'import torch import torch.nn as nn from tqdm import tqdm import pandas as pd'),\n",
       " ('../deep_learning\\\\08-mlp.ipynb',\n",
       "  2,\n",
       "  'code',\n",
       "  'def train_model(model, X, y, lr=0.01, epochs=100):          criterion = nn.MSELoss()     optimizer = torch.optim.Adam(model.parameters(), lr=lr)      # Training loop     outputs = [X.numpy()]     for '),\n",
       " ('../deep_learning\\\\08-mlp.ipynb',\n",
       "  13,\n",
       "  'markdown',\n",
       "  \"In the animation, we see that each part of the space is being folded and bended differenty. This is because of the non-linearity given by the ReLU function.  ### A very small example  Let's suppose we\"),\n",
       " ('../deep_learning\\\\08-mlp.ipynb',\n",
       "  21,\n",
       "  'code',\n",
       "  'x1 = torch.linspace(-1, 1, 500) x2 = torch.linspace(1, -1, 500) X = torch.stack([x1, x2], dim=1) X += 0.1 * torch.randn(500, 2)  # Adding noise to X  y = 2*X + X**2 +0.5  import matplotlib.pyplot as p'),\n",
       " ('../deep_learning\\\\09-sequence_models.ipynb',\n",
       "  2,\n",
       "  'markdown',\n",
       "  '# Multi-Head Attention Encoders  If you were not living under a rock in the last few years, you probably heard about something called a \"transformer\". This is a neural network topology made famous in '),\n",
       " ('../deep_learning\\\\09-sequence_models.ipynb',\n",
       "  4,\n",
       "  'code',\n",
       "  'import torch import torch.nn as nn  def _make_causal_mask(seq_len):     # The mask is True where we want to ignore attention and False elsewhere.     mask = torch.ones(seq_len, seq_len)     mask = tor')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def keyword_search_in_notebooks(paths: list, keyword: str) -> list:\n",
    "    \"\"\"\n",
    "    Para cada caminho em paths, busca keyword (case-insensitive) em cada célula.\n",
    "    Retorna lista de tuplas: (arquivo, índice_da_célula, tipo, snippet).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for path in paths:\n",
    "        cells = load_notebook_cells(path)\n",
    "        for idx, cell in enumerate(cells):\n",
    "            source = ''.join(cell.get('source', []))\n",
    "            if keyword.lower() in source.lower():\n",
    "                results.append((path, idx, cell['cell_type'], source[:200].replace('\\n', ' ')))\n",
    "    return results\n",
    "\n",
    "# todos os notebooks da pasta '../deep_learning'\n",
    "notebooks = [os.path.join('../deep_learning', f) for f in os.listdir('../deep_learning') if f.endswith('.ipynb')]\n",
    "keyword_search_in_notebooks(notebooks, 'import')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d07b763",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "There is an inherent fragility in the previous system: it requires the user to guess the keyword correctly. Trivia fact: in the early 2000s, the hability to guess keywords in Google had the same hype that using AI chatbots has nowadays.\n",
    "\n",
    "We could prevent our user from trying to guess the exact keyword or phrase, afterall, we have an estimator for phrase similarity: BERT!\n",
    "\n",
    "Improve your code so that it:\n",
    "\n",
    "1. Collects a phrase from the user,\n",
    "1. Calculates the phrase embedding $q$ using the CLS token from BERT\n",
    "1. Traverses the course material calculating the embedding $x_i$ for each cell\n",
    "1. Finds the $k$ (try with $k=1$, then generalize to any $k$) cells with minimal cosine distance ($d = \\frac{ <q, x_1>}{||x|| ||c_i||}$) with relationship to the phrase.\n",
    "\n",
    "Reflect: was this a better choice for retrieval? How can we measure this difference? (tip: research how information retrieval systems are evaluated!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c827a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.8999\n",
      "Tipo: markdown\n",
      "Conteúdo: ## Making a pipeline with PyTorch\n",
      "\n",
      "Score: 0.8927\n",
      "Tipo: markdown\n",
      "Conteúdo: ## \n",
      "\n",
      "Score: 0.8798\n",
      "Tipo: markdown\n",
      "Conteúdo: ### Evaluating the model\n",
      "\n",
      "We can convert our \n",
      "\n",
      "Score: 0.8332\n",
      "Tipo: markdown\n",
      "Conteúdo: ## Visualizing embeddings\n",
      "\n",
      "Score: 0.8321\n",
      "Tipo: markdown\n",
      "Conteúdo: # Modelling sequences\n",
      "\n",
      "Score: 0.8212\n",
      "Tipo: markdown\n",
      "Conteúdo: \n",
      "\n",
      "```mermaid\n",
      "graph LR;\n",
      "    subgraph Inputs;\n",
      "    INPUT[\"[CLS]\n",
      "        remove\n",
      "        some\n",
      "        parts\n",
      "        [MASK]\n",
      "        a\n",
      "        sentence\"];\n",
      "    end;\n",
      "    INPUT --> BERT[\"BERT\"];\n",
      "    subgraph Outputs;\n",
      "    OUTPUT[\"C\n",
      "    T1\n",
      "    T2\n",
      "    T3\n",
      "    T4\n",
      "    T5\n",
      "    T6\"];\n",
      "    end;\n",
      "    BERT --> OUTPUT;\n",
      "    Train[\"Loss: T4 should be the word 'of'\"]\n",
      "    OUTPUT --- Train;\n",
      "```\n",
      "\n",
      "\n",
      "This task suggests that the embedding space created by BERT should allow representing words in the context of the rest of the sentence!\n",
      "\n",
      "To play with this task with Hugging Face's library, you can use:\n",
      "\n",
      "\n",
      "\n",
      "Score: 0.7951\n",
      "Tipo: markdown\n",
      "Conteúdo: ## Exercise\n",
      "\n",
      "Our usual way to approach classification is to do something in the lines of:\n",
      "\n",
      "Score: 0.7934\n",
      "Tipo: markdown\n",
      "Conteúdo: \n",
      "The embedding for the [CLS] token can be accessed using:\n",
      "\n",
      "\n",
      "Score: 0.7813\n",
      "Tipo: markdown\n",
      "Conteúdo: ### Question 1: What is the underlying premise of the Bag-of-Words classifier, that is, why does BoW allow to classify these texts?\n",
      "\n",
      "Score: 0.7750\n",
      "Tipo: markdown\n",
      "Conteúdo: ### Example code to test the trained tokenizer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Carrega BERT (PyTorch)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "def embed_text(text: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Retorna embedding do texto usando o token CLS do BERT.\n",
    "    \"\"\"\n",
    "    # Tokenização e conversão para tensores PyTorch\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # CLS embedding é a primeira posição\n",
    "    cls_emb = outputs.last_hidden_state[0, 0, :]\n",
    "    return cls_emb.cpu().numpy().squeeze()\n",
    "\n",
    "\n",
    "def semantic_search(cells: list, query: str, k) -> list:\n",
    "    \"\"\"\n",
    "    Retorna as k células cujos embeddings são mais similares ao query.\n",
    "    \"\"\"\n",
    "    q_emb = embed_text(query).reshape(1, -1)\n",
    "    similarities = []\n",
    "\n",
    "    for cell in cells:\n",
    "        if cell['cell_type'] == 'markdown':\n",
    "            text = ''.join(cell.get('source', []))\n",
    "            emb = embed_text(text).reshape(1, -1)\n",
    "            score = cosine_similarity(q_emb, emb)[0, 0]\n",
    "            similarities.append((cell, float(score)))\n",
    "\n",
    "    # Ordena decrescente e retorna top k\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:k]\n",
    "\n",
    "all_notebooks = [os.path.join('../deep_learning', f) for f in os.listdir('../deep_learning') if f.endswith('.ipynb')]\n",
    "# Carrega todas as células de todos os notebooks\n",
    "cells = []\n",
    "for path in all_notebooks:\n",
    "    cells.extend(load_notebook_cells(path))\n",
    "# Exemplo de busca semântica\n",
    "query = \"BERT\"\n",
    "top_results = semantic_search(cells, query, k=10)\n",
    "for cell, score in top_results:\n",
    "    print(f\"Score: {score:.4f}\\nTipo: {cell['cell_type']}\\nConteúdo: {''.join(cell['source'])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5954f5",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "Now let's leave our retrival system waiting for a while.\n",
    "\n",
    "Make a small program that:\n",
    "\n",
    "1. Collects a question from the user\n",
    "1. Uses an API to redirect this question to an LLM, and immediately returns the answer.\n",
    "1. Add prompt information so that your answers can only regard NLP-related subjects (these are called \"safeguards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "297b9b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: models/gemini-1.0-pro-vision-latest\n",
      "Modelo: models/gemini-pro-vision\n",
      "Modelo: models/gemini-1.5-pro-latest\n",
      "Modelo: models/gemini-1.5-pro-001\n",
      "Modelo: models/gemini-1.5-pro-002\n",
      "Modelo: models/gemini-1.5-pro\n",
      "Modelo: models/gemini-1.5-flash-latest\n",
      "Modelo: models/gemini-1.5-flash-001\n",
      "Modelo: models/gemini-1.5-flash-001-tuning\n",
      "Modelo: models/gemini-1.5-flash\n",
      "Modelo: models/gemini-1.5-flash-002\n",
      "Modelo: models/gemini-1.5-flash-8b\n",
      "Modelo: models/gemini-1.5-flash-8b-001\n",
      "Modelo: models/gemini-1.5-flash-8b-latest\n",
      "Modelo: models/gemini-1.5-flash-8b-exp-0827\n",
      "Modelo: models/gemini-1.5-flash-8b-exp-0924\n",
      "Modelo: models/gemini-2.5-pro-exp-03-25\n",
      "Modelo: models/gemini-2.5-pro-preview-03-25\n",
      "Modelo: models/gemini-2.5-flash-preview-04-17\n",
      "Modelo: models/gemini-2.5-flash-preview-04-17-thinking\n",
      "Modelo: models/gemini-2.5-pro-preview-05-06\n",
      "Modelo: models/gemini-2.0-flash-exp\n",
      "Modelo: models/gemini-2.0-flash\n",
      "Modelo: models/gemini-2.0-flash-001\n",
      "Modelo: models/gemini-2.0-flash-exp-image-generation\n",
      "Modelo: models/gemini-2.0-flash-lite-001\n",
      "Modelo: models/gemini-2.0-flash-lite\n",
      "Modelo: models/gemini-2.0-flash-preview-image-generation\n",
      "Modelo: models/gemini-2.0-flash-lite-preview-02-05\n",
      "Modelo: models/gemini-2.0-flash-lite-preview\n",
      "Modelo: models/gemini-2.0-pro-exp\n",
      "Modelo: models/gemini-2.0-pro-exp-02-05\n",
      "Modelo: models/gemini-exp-1206\n",
      "Modelo: models/gemini-2.0-flash-thinking-exp-01-21\n",
      "Modelo: models/gemini-2.0-flash-thinking-exp\n",
      "Modelo: models/gemini-2.0-flash-thinking-exp-1219\n",
      "Modelo: models/learnlm-1.5-pro-experimental\n",
      "Modelo: models/learnlm-2.0-flash-experimental\n",
      "Modelo: models/gemma-3-1b-it\n",
      "Modelo: models/gemma-3-4b-it\n",
      "Modelo: models/gemma-3-12b-it\n",
      "Modelo: models/gemma-3-27b-it\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "models = list(genai.list_models())\n",
    "for m in models:\n",
    "    # verificar quais modelos sao generativeContent (verificar lista supported_generation_methods por 'generateContent')\n",
    "    if 'generateContent' in m.supported_generation_methods:\n",
    "        print(f\"Modelo: {m.name}\")\n",
    "\n",
    "\n",
    "# Para ver os modelos disponíveis:\n",
    "# print(genai.list_models())\n",
    "\n",
    "generation_config = genai.GenerationConfig(\n",
    "    max_output_tokens=500,\n",
    "    temperature=1.0,\n",
    ")\n",
    "gemini_model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
    "\n",
    "def llm_query(question: str, domain: str = 'NLP') -> str:\n",
    "    \"\"\"\n",
    "    Envia question ao Gemini, com system prompt restringindo ao domínio.\n",
    "    \"\"\"\n",
    "    system_prompt = (\n",
    "        f\"Você é um assistente especializado em {domain}. \"\n",
    "        \"Responda somente usando informações desse domínio.\"\n",
    "    )\n",
    "    full_prompt = system_prompt + \"\\n\\n\" + question\n",
    "    response = gemini_model.generate_content(\n",
    "        full_prompt,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be101bb",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "Now, let's joint everything.\n",
    "\n",
    "We are able to find specific information from our courseware. Also, we are able to use LLMs. Use both abilities to:\n",
    "\n",
    "1. Collect a question from the user\n",
    "1. Retrieve the $K$ most relevant cells from the course material\n",
    "1. Use the content of these cells as part of a prompt. The prompt includes both the question and the content from the relevant cells.\n",
    "1. Phrase your prompt so that the LLM can only return information that is contained in the course material.\n",
    "\n",
    "Reflect: how does this compare to the system in Exercise 4? How can we measure the differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a2c6f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n",
      "=== CONTEXTO ===\n",
      "### Question 1: What is the underlying premise of the Bag-of-Words classifier, that is, why does BoW allow to classify these texts?\n",
      "### Question 2: What is the underlying premise of a BERT-based classifier, that is, why should BERT embeddings be interesting to classify these texts?\n",
      "\n",
      "## Making a pipeline with PyTorch\n",
      "### Evaluating the model\n",
      "\n",
      "We can convert our \n",
      "## Visualizing embeddings\n",
      "### Example code to test the trained tokenizer:\n",
      "## Exercise\n",
      "\n",
      "Our usual way to approach classification is to do something in the lines of:\n",
      "In fact, we could simply decide our class by thresholding our output:\n",
      "# Further reading\n",
      "\n",
      "We usually think that the last layer of the language model should be used in downstream tasks. However, [this paper](https://arxiv.org/abs/2502.02013) indicates that this is not always true! Check the paper - and see how it applies to the language model you have previously trained!\n",
      "\n",
      "The embedding for the [CLS] token can be accessed using:\n",
      "\n",
      "## Exercise\n",
      "\n",
      "Make a neural network that maps $X$ to $y$ using the data below. Plot an animation of the convergence process, like the ones shown above. Try the different model variations - what happens in each case?\n",
      "\n",
      "\n",
      "```mermaid\n",
      "graph LR;\n",
      "    subgraph Inputs;\n",
      "    INPUT[\"[CLS]\n",
      "        remove\n",
      "        some\n",
      "        parts\n",
      "        [MASK]\n",
      "        a\n",
      "        sentence\"];\n",
      "    end;\n",
      "    INPUT --> BERT[\"BERT\"];\n",
      "    subgraph Outputs;\n",
      "    OUTPUT[\"C\n",
      "    T1\n",
      "    T2\n",
      "    T3\n",
      "    T4\n",
      "    T5\n",
      "    T6\"];\n",
      "    end;\n",
      "    BERT --> OUTPUT;\n",
      "    Train[\"Loss: T4 should be the word 'of'\"]\n",
      "    OUTPUT --- Train;\n",
      "```\n",
      "\n",
      "\n",
      "This task suggests that the embedding space created by BERT should allow representing words in the context of the rest of the sentence!\n",
      "\n",
      "To play with this task with Hugging Face's library, you can use:\n",
      "\n",
      "\n",
      "    \n",
      "There are many details in this implementation, so I made a [video exploring them all](https://youtu.be/FXtGq_TYLzM).\n",
      "# Modelling sequences\n",
      "## \n",
      "\n",
      "Now, change \"man\" for \"woman\". The result is not as pretty. But, see, this is not a problem of the language model structure per se - rather, it is a problem of the data used to train it.\n",
      "\n",
      "We could go on finding examples of other types of prejudice - there are all sorts of sexism and racism lying in the hidden spaces of BERT.\n",
      "\n",
      "This is bad, but remember this was 2019, and people were impressed that the system could generate coherent words at all! Nowadays, LLM outputs go through a filter that finds phrases that are potentially harmful, so they don't write ugly phrases.\n",
      "\n",
      "Which of the phrases below are true about this?\n",
      "# Logistic Regression with Pytorch\n",
      "\n",
      "Allright, everyone. Let's go ahead and start doing things. We will start with a brief review, so that we are all on the same page.\n",
      "\n",
      "## Review: Logistic Regression with Scikit-Learn\n",
      "\n",
      "If you have reached this point, you probably know by heart how to define a Logistic Regression pipeline in Scikit Learn:\n",
      "\n",
      "Now, we have two nested training loops: we need to train for each epoch, and, within each epoch, we need to train for each batch. Check the code below. I commented some lines so you don't accidentally run it.\n",
      "In fact, we could go ahead and have our model trained, so that the next steps in this lesson are going to be a bit smoother. We could fit our model to classify some texts with our usual dataset:\n",
      "## PART 1: preparation\n",
      "\n",
      "Answer the questions below as comments in the following cell. In your answers, avoid using common sense. Use adequate jargons.\n",
      "## Baseline classifier\n",
      "\n",
      "We will start with a baseline classifier. It is a simple Bag-of-words classifier.\n",
      "Note that our `output_probs` here is equal to the second column of the output of `classifier.predict_proba()`:\n",
      "### Algorithmic bias and Hallucinations\n",
      "\n",
      "Note that BERT is generating words that make sense. However, these continuations do not necessarily correspond to reality. In fact, these continuations are simply something that maximizes a probability related to a specific dataset!\n",
      "\n",
      "Check, for example, the output for:\n",
      "\n",
      "A linear layer takes something as an input $X$, multiplies it by a weight matrix $w$ and sums a bias $b$. In other words:\n",
      "\n",
      "$$\n",
      "z = \\beta_0 + X w^T\n",
      "$$\n",
      "\n",
      "Sounds familiar? Let's see this operation working in practice:\n",
      "\n",
      "A linear layer takes something as an input $X$, multiplies it by a weight matrix $w$ and sums a bias $b$. In other words:\n",
      "\n",
      "$$\n",
      "z = \\beta_0 + X w^T\n",
      "$$\n",
      "\n",
      "Sounds familiar? Let's see this operation working in practice:\n",
      "# BERT\n",
      "\n",
      "**GOAL: At the end of this class, we will be able to USE a pre-trained BERT to (a) generate suggestions and to (b) generate embeddings for classification**\n",
      "\n",
      "\n",
      "## Task 2: Next Sentence Prediction\n",
      "\n",
      "BERT was also trained for a task called Next Sentence Prediction. The idea of this task is to insert two sentences in the input of BERT, separating them with a special [SEP] token. Then, the system uses the output of the [CLS] token to classify whether these two sentences do or do not follow each other. It is something like:\n",
      "\n",
      "```mermaid\n",
      "graph LR;\n",
      "    subgraph Inputs;\n",
      "    INPUT[\"[CLS]\n",
      "        Here\n",
      "        I\n",
      "        am\n",
      "        [MASK]\n",
      "        rock\n",
      "        you\n",
      "        like\n",
      "        a\n",
      "        hurricane\"];\n",
      "    end;\n",
      "    INPUT --> BERT[\"BERT\"];\n",
      "    subgraph Outputs;\n",
      "    OUTPUT[\"C\n",
      "    T1\n",
      "    T2\n",
      "    etc\"];\n",
      "    end;\n",
      "    BERT --> OUTPUT;\n",
      "    Train[\"Loss: C should be equal to 1\"]\n",
      "    OUTPUT --- Train;\n",
      "```\n",
      "\n",
      "```mermaid\n",
      "graph LR;\n",
      "    subgraph Inputs;\n",
      "    INPUT[\"[CLS]\n",
      "        Here\n",
      "        I\n",
      "        am\n",
      "        [MASK]\n",
      "        rock\n",
      "        your\n",
      "        body\"];\n",
      "    end;\n",
      "    INPUT --> BERT[\"BERT\"];\n",
      "    subgraph Outputs;\n",
      "    OUTPUT[\"C\n",
      "    T1\n",
      "    T2\n",
      "    etc\"];\n",
      "    end;\n",
      "    BERT --> OUTPUT;\n",
      "    Train[\"Loss: C should be equal to 0\"]\n",
      "    OUTPUT --- Train;\n",
      "```\n",
      "\n",
      "The consequence of this training is that the embedding $C$ of the [CLS] token represents the content of the rest of the tokens. Hence, we can use it for classification. For such, we can go straight to the HuggingFace library and use:\n",
      "\n",
      "\n",
      "# ACTIVITY: A CLASSIFIER\n",
      "\n",
      "The goal of this activity is to practice building and discussing a classifier. By the end of the activity, you should be able to justify your design decisions according to \n",
      "\n",
      "TASK:\n",
      "In the next prediction, we concatenate the input at step $1$ with the output at the step $0$, that is, we use $x_e = [E_{[1,:,:]}, y_0]$ and get another prediction $y_1$. Then we keep doing this for all time steps $t \\in [0, l-1]$. The network starts looking like this:\n",
      "* Classifica de acordo com a contagem de palavras sem levar em consideração o contexto, então se uma palavra aparece dentro de uma categoria muitas vezes ela vai ter mais peso\n",
      "* Tfidf leva e consideração a raridade da palavra no dataset\n",
      "\n",
      "Em BoW, cada palavra do vocabulário é representada como um valor em uma dimensão de um vetor que representa o documento como um todo. No processo de classificação, cada uma dessas dimensões é multiplicada por um peso específico, gerando os logits para classificação. Isso permite classificar textos de acordo com a presença ou ausência de determinadas palavras. A vantagem de BoW é que é fácil interpretar, pois sabemos quais palavras foram usadas para a classificação. A sua principal desvantagem é que não leva em consideração o contexto em que cada palavra aparece.\n",
      "How does pytorch know that gradients should be calculated? Well, they are calculated by default. That is why, in inference (or: in testing), we use `torch.no_grad()` and `model.eval()` - this saves memory, and saves computation time as well:\n",
      "## Our first model with Pytorch\n",
      "\n",
      "If you haven't done so, this is the moment to `pip install torch`. Pytorch is a framework that provides an API similar to Numpy with the addition of allowing operations in the GPU and easily using Autograd. Also, it contains many classes that are very useful for applications in Machine Learning. For example:\n",
      "\n",
      "\n",
      "Kentucky is a state in the USA that may or may not have wineries, but definitely does not have famous beaches! Now, check the output when you change Kentucky for the Brazilian state of Minas Gerais!\n",
      "\n",
      "See - there is no \"brain\" inside BERT. There is merely a system that finds plausible completions for a task. This is something we have been calling \"hallucinations\" in LLMs. In the end, the model is just as biased as the dataset used for training it.\n",
      "\n",
      "### Algorithmic prejudice\n",
      "\n",
      "Despite the funny things things that the model could output, there are some assertions that can be dangerous, or outright sexist. Try to see the output of:\n",
      "\n",
      "Now, we can run the linear layer with `X_vect` and then apply a logistic function:\n",
      "## Exercise: a Language Model \n",
      "\n",
      "In the file `train_language_model.py`, there is code to train a language model. It should run relatively fast because it only uses a small, toy dataset.\n",
      "\n",
      "Check the code, and possibly the [figure with embeddings](word_embeddings.png) and identify:\n",
      "\n",
      "1. On training, what is the model trying to predict?\n",
      "1. What is the similarity between this architecture and a regular classifier?\n",
      "1. After training, why are some word embeddings so close to each other?\n",
      "1. After the sequence model, what would happen to embeddings of phrases that use words that are close in the word embedding space?\n",
      "1. If we use a logistic regression (LR) in a labeled dataset that links \"cats\" to a label \"positive\", but it has no mention of dogs, what would happen if we use this LR to predict the label for \"dogs\"? Why?\n",
      "1. Common sense says: \"embeddings learn and capture the semantic relationships between words\". Rephrase this using more precise terms: what are embeddings really modelling, from a purely statistical/mathematical point of view?\n",
      "1. Training the model with so few examples is already taking a significant amount of time. What could we do to mitigate this?\n",
      "\n",
      "BERT foi pré-treinado para resolver a tarefa close e a tarefa de predição de próxima sentença. A consequência disso é que BERT gera embeddings densos para cada documento e documentos que geram respostas próximas nestas tarefas estão mapeados para posições próximas no espaço de embeddings. Em alguns contextos, essa procimidade é interpretada como o mapeamento de proximidades semânticas para proximidades no espaço euclidiano. Por isso, os embeddings de BERT podem ser usados em uma Regressão Logística para classificar documentos. A vantagem dos embeddings de BERT é que os embeddings densos não dependem de treinamento de vocabulário específico para cada tarefa. A desvantagem é que é mais difícil de interpretar esses embeddings, uma vez que não podemos acessar exatamente o que leva uma frase a ser mapeada sobre um ponto específico. Além disso, BERT depende de um pré-treino, e, portanto, é limitado aos contextos em que foi pré-treinado.\n",
      "# Summary\n",
      "\n",
      "If everything went well, you should know how to:\n",
      "\n",
      "1. Make a PyTorch model\n",
      "1. Use RNNs to summarize the content of sequences\n",
      "1. Train neural networks with RNNs\n",
      "1. Use the GPU to speed-up training\n",
      "1. Visualize the learning curve and diagnose failures with the learning rate\n",
      "1. Visualize document-level embeddings to check the final state of training\n",
      "Remember that in Scikit-Learn we had pipelines? In PyTorch, the equivalent procedure is to make a class. This class should inherit from `nn.Module` and we have to define, at least, the methods:\n",
      "\n",
      "* `__init__`, which will initialize the class, instantiate all blocks that have parameters, and so on;\n",
      "* `forward`, which is called when the object is called (it bothers me that the more pythonic `__call__` was not chosen for this...). This method implements the actual workings of the pipeline. See this example:\n",
      "### Exercise 1\n",
      "\n",
      "In the formulation for the linear layer above, $z$ corresponds to an important part of a logistic regression (remember: logistic regression has an input, a decision function, and then a final probability estimate $P(C | x)$). Which of these parts corresponds to the linear layer operation, and what is missing to make a full logistic regression using the linear layer?\n",
      "## Watching our training process\n",
      "\n",
      "If you simply increased the number of epochs to 1000 in our loop, you probably had an overflow of `print` statements with the current loss. We probably don't want to see that - rather, we want a figure!\n",
      "\n",
      "Up until a few years ago, we had to make this figure all by ourselves, with a procedure such as:\n",
      "\n",
      "## A more complicated dataset: linear by parts\n",
      "\n",
      "Now, let's get a more complicated dataset. Now, $X$ (our input) will have three different clusters, and we will apply a different linear transform in each cluster.\n",
      "\n",
      "\n",
      "# Exercise\n",
      "\n",
      "Compare three different approaches to tackle the classification problem for texts in Portuguese. For each problem, make a learning curve, that is, make random samples of the training dataset with different sizes.\n",
      "\n",
      "1. Perform classification directly in Portuguese, using a Bag-of-Words approach\n",
      "1. Classify texts directly in Portuguese using multilingual BERT\n",
      "1. Translate texts to English and then use the usual BERT\n",
      "\n",
      "Remember that when we call `pipeline.fit()`, we first fit the vectorizer and then, with the results, we fit the classifier. So, these two methods for training our model are absolutely equivalent:\n",
      "The Pytorch implementation of RNN is simply a layer like:\n",
      "\n",
      "    rnnlayer = nn.RNN(input_size, hidden_size)\n",
      "\n",
      "We could change our network from the last lesson to:\n",
      "# ACTIVITY: LOST IN TRANSLATION\n",
      "\n",
      "**ACTIVITY DESCRIPTION: At the end of this activity, the learner will be able to compare different methodologies for classification of non-English texts**\n",
      "\n",
      "**WHAT YOU SHOULD FOCUS ON: Use and compare different methodologies for a similar task**\n",
      "\n",
      "One (very big) problem of natural language processing is that most data *corpi* are in English, but most people in the world are not English speakers. There are languages that have few speakers, or that have few available documents for training. This is especially true in the context of large-scale models, which are known for being *data hungry*, that is, they require *a lot* of data to be adequately trained.\n",
      "\n",
      "There are essentially two methods to deal with non-English texts:\n",
      "\n",
      "1. Translate the text to English and then act on the translation\n",
      "1. Train language models in different languages\n",
      "\n",
      "Nowadays, translation is relatively easy to perform using the `argostranslate` package in Python:\n",
      "\n",
      "\n",
      "When we go from sequences of word embeddings to a document-wise vector representation that can be classified, we have to somehow summarize a sequence of vectors into a single vector. So far, what we have been doing is:\n",
      "\n",
      "1. Get one embedding per word ($\\mathbb{t \\times n}$, where $t$ is the sequence length and $n$ is the embedding dimension),\n",
      "1. Calculate the timewise mean of the words ($\\mathbb{1 \\times n}$)\n",
      "1. Proceed to classification with our Residual MLP modules\n",
      "\n",
      "This is something like this:\n",
      "\n",
      "<img src=\"figs/classifier.png\" />\n",
      "\n",
      "The problem with this idea is that the calculation of the mean totally disregards the order of the words - essentially, we are doing a glorified bag-of-words modelling, which seems non-ideal. When we do so, we are We could find some other way to summarize our sequence of words so that we somehow account for the order of words.\n",
      "\n",
      "\n",
      "\n",
      "Nowadays, we have plenty of frameworks to store this same data: there is Weights and Biases, MLFlow, TensorBoard, and so on. Please, do feel free to use any of them. We will not adopt any of them for this course because we might get trapped within very specific details of them.\n",
      "\n",
      "\n",
      "**The main lessons here are:**\n",
      "\n",
      "* Save your data to a variable, and plot figures later (do NOT plot while doing training!)\n",
      "* Do not use the terminal to debug your loss\n",
      "* Use `tqdm` so we know your code is running.\n",
      "\n",
      "## Exercise 2\n",
      "\n",
      "Manipulate the code above to find the following:\n",
      "\n",
      "1. When we use `torch.sum( (output_probs-y_train_vect)**2 )` to calculate the loss, we are essentially saying that larger datasets have larger losses. Change this calculation so the loss is independent of the number of items in the dataset.\n",
      "1. You might want to increase the learning rate `lr` to make your training faster. What happens if you increase it too much? Can you guess why?\n",
      "1. Change your code to show that the `accuracy` (at least in the training set) tends to decrease together with the `loss`.\n",
      "1. How many epochs do you actually need in this training process?\n",
      "## Exercise (do it at home, it takes a while!)\n",
      "\n",
      "1. Find some text you like\n",
      "1. Use the `Dataset` and `Dataloader` from Pytorch to make it accessible by your transformer\n",
      "1. Train your model!\n",
      "1. Try to predict and reinsert some tokens/words in your model\n",
      "1. Use the embeddings in a downstream task (e.g., classification)\n",
      "## What is BERT?\n",
      "\n",
      "After the [transformer](https://arxiv.org/abs/1706.03762), we had many other advances. One of such, of course, is the [GPT](https://paperswithcode.com/paper/improving-language-understanding-by), which uses a decoder-only transformer architecture to predict the next word in a sentence. GPT uses a decoder-only architecture because it needs the masked multi-head attention device to avoid making trivial predictions. Ultimately, GPT generates an embedding space that increases the likelihood of choosing meaningful words for a text continuation.\n",
      "\n",
      "The Google team found another interesting way to obtain this type of representation. They trained an *encoder*-only transformer that can predict words removed from the text - similarly to how we know what is missing in  \"Luke, I am your ____\". The idea here is that we can use information from the future for this task, because it is highly dependent on context. Simultaneously, they trained the model to classify whether two given phrases follow each other in a corpus. So, BERT was born.\n",
      "## Strategies for tokenization\n",
      "\n",
      "Just some minutes ago, we discussed that a token corresponds to a word. In fact, a token is a piece of the text that conveys some information - and this could be a word. Also, we have used n-grams as tokens when we wanted to consider the sequence of words.\n",
      "\n",
      "Now, let's remember. If we had around, maybe, $10^4$ words in our vocabulary, then we would have potentially $10^8$ bi-grams, and $10^{12}$ potential tri-grams. This is because we are aggreting more and more possible symbols.\n",
      "\n",
      "However, we could go on the opposite direction here.\n",
      "\n",
      "If we had the words \"working\", \"works\", \"walking\", \"walks\", \"calling\", and \"calls\", we could easily tokenize them into 6 tokens, each corresponding to full words.\n",
      "\n",
      "However, we could separate the suffixes and the stems. In this case, we could have tokens for: \"work\", \"walk\", \"call\", \"s\", and \"ing\". See: now we have 5 tokens instead of 6.\n",
      "\n",
      "This is generally called a \"subword\" tokenization strategy. One of the most used algorithms for such is called Sentence Piece Tokenizer. It works by building a vocabulary like this:\n",
      "\n",
      "1. Assume each character is a separate token\n",
      "1. Create a new token that merges the most common token sequence into a separate token\n",
      "1. Repeat the merging until we reach a reasonable vocabulary size\n",
      "\n",
      "A nice advantage of using this method is that it is less likely to find unknown tokens - afterall, we start from a limited vocabulary.\n",
      "\n",
      "Also, we are not going to build a tokenizer from scratch: instead, we are going to use a ready-made one. If you want to read all about it, refer to:\n",
      "\n",
      "[Kudo, T., and Richardson, J. SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (2018)](https://arxiv.org/pdf/1808.06226)\n",
      "\n",
      "### Example code for training a Sentence Piece tokenizer:\n",
      "Now, instead of using a TfIdf vectorizer, calculate embeddings for the texts in the dataset using BERT. Then, use *them* to classify. Compare the results with the ones we have when we use the Bag-of-words approach.\n",
      "\n",
      "Justify these results using the concept of embeddings we have studied in the previous lessons.\n",
      "```mermaid\n",
      "graph LR;\n",
      "\n",
      "    subgraph Embedding;\n",
      "    D([\"Embeddings ($$E \\in \\mathbb{R}^{b \\times l \\times d}$$)\"])\n",
      "    end;\n",
      "\n",
      "    subgraph Summarization;\n",
      "    D --> E[\"Mean over time\"] --> F([\"Embeddings ($$X \\in \\mathbb{R}^{b \\times d}$$)\"])\n",
      "    end;\n",
      "\n",
      "    subgraph Classification;\n",
      "    F --> G[\"Logistic Regression\"] --> H([\"$$P(C_i=c_j | X_i)$$\"])\n",
      "    end;\n",
      "```\n",
      "Note that, in the code above, we yielded a batch of three texts to the embedding layer. The batch has a length of four. Then, the embedding layer returns a tensor with tridimensional shape. The shape $3 \\times 4 \\times 2$ means:\n",
      "\n",
      "* We have 3 documents\n",
      "* Each document has 4 tokens\n",
      "* Each word is encoded in 2 dimensions\n",
      "\n",
      "\n",
      "Bert stands for [Bidirectional Encoder Representations from Transformers, and was introduced in this paper from 2019](https://arxiv.org/pdf/1810.04805). The greatest contribution of BERT, besides its architecture, is the idea of training the language model for different tasks at the same time.\n",
      "\n",
      "We are definitely not going to train BERT in class, but we are using it for other tasks. We will use the [BERT implementation from Hugging Face](https://huggingface.co/google-bert/bert-base-uncased). All help files are here.\n",
      "\n",
      "## Task 1: Masked Language Model\n",
      "\n",
      "The first task BERT was trained for was the Masked Language Model. This was inspired in a task called [\"Cloze\"](https://en.wikipedia.org/wiki/Cloze_test), and the idea is to remove a word from a sentence and let the system predict what word should fill that sentence:\n",
      "\n",
      "\n",
      "## Exercises\n",
      "\n",
      "### 1 - Visualizing embeddings\n",
      "\n",
      "Change the code for the `SimpleClassifier` class by adding a method that returns the word-level embeddings. Then, calculate the embedding for each word in your vocabulary. Plot the embeddings before training the model, and then after training the model. Do you see a pattern there?\n",
      "\n",
      "### 2 - Further optimization\n",
      "\n",
      "Review the whole code for the classifier. Identify all parameters you can change. Identify the possible performance bottlenecks, change the parameters to reduce these bottlenecks, and then run the training process again. Do you see changes?\n",
      "\n",
      "### 3 - Advanced modelling\n",
      "\n",
      "Change the `forward` method in `SimpleClassifier` so that the mean only accounts for non-PAD tokens (that is: PAD tokens are ignored). Does that improve performance?\n",
      "```mermaid\n",
      "graph LR;\n",
      "    F([\"Embeddings ($$E \\in \\mathbb{R}^{b \\times l \\times d}$$)\"])\n",
      "    FF([\"$$x_e = [E_{[t,:,:]}, y_{t-1}]$$\"])\n",
      "\n",
      "    F-->FF\n",
      "    FF --> G[\"$$\\tanh(xw^T+b)$$\"] \n",
      "\n",
      "    G --> H([$$y_t$$])\n",
      "\n",
      "    H -- \"Feedback loop\" --> FF\n",
      "```\n",
      "    \n",
      "Ok, now we are going to get into an optimization loop. When using Pytorch, we must first define an optimizer - we will use SGD, which is literally the gradient descent algorithm we have seen so far. Then, we will go into a training loop consisting of:\n",
      "\n",
      "1. Zeroing the gradient in the optimizer to reset its state,\n",
      "1. Calculate the output of the classifier\n",
      "1. Claculate the loss related to the output\n",
      "1. Calculate (going back into the layer!) the gradient of the loss using [autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)\n",
      "1. Apply the gradient to our parameters\n",
      "\n",
      "It goes like this:\n",
      "\n",
      "Remeber that the multihead attention models sequence dependencies and yields:\n",
      "\n",
      "$$\n",
      "y = \\text{softmax}(QK^T/\\sqrt{D})V.\n",
      "$$\n",
      "\n",
      "The problem of this is that $QK^T$ is non-causal, that is, tokens get it touch with other, future tokens, thus prediction using this would be trivial again.\n",
      "\n",
      "For such, we use a *mask*. It works like this:\n",
      "\n",
      "$(QK^T)_{i,j}$ represents the attention given to word $i$ due to word $j$. Hence, we have two restrictions:\n",
      "\n",
      "1. We cannot have word $i$ depend on word $j$ for any $j>i$,\n",
      "1. We still need the $\\text{softmax}$ operator because we want to make a weighted mean of the elements in $V$.\n",
      "\n",
      "The solution for such is to make a matrix $M$ with the same shape as $QK^T$. The elements of this matrix are:\n",
      "\n",
      "$$\n",
      "m_{i,j} = \n",
      "\\left\\{\n",
      "\\begin{array}{rl}\n",
      "0,& i \\leq j \\\\\n",
      "-\\infty, & i > j  \n",
      "\\end{array}\n",
      "\\right.\n",
      "$$\n",
      "\n",
      "If we had three tokens in the sequence, we would get something like:\n",
      "\n",
      "$$\n",
      "M = \n",
      "\\begin{bmatrix}\n",
      "0 & -\\infty & -\\infty \\\\\n",
      "0 & 0 & -\\infty \\\\\n",
      "0 & 0 & 0 \\\\\n",
      "\\end{bmatrix}\n",
      "$$\n",
      "\n",
      "Ok, then we add this matrix to $QK^T$ *before* the softmax operation. Hence, we get our masked output:\n",
      "\n",
      "$$\n",
      "y = \\text{softmax}((M+QK^T)/\\sqrt{D})V.\n",
      "$$\n",
      "\n",
      "See that after the $\\text{softmax}$ normalization, the $-\\infty$ terms go to zero, whereas the other elements still sum to one.\n",
      "\n",
      "Lets implement this mask in Pytorch:\n",
      "\n",
      "It seems to be a nice idea to use masked multi-head self-attention as a sequence model!\n",
      "\n",
      "According to the [pytorch documentation](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html), the attention mask should be given as a parameter called `attn_mask` when we call the forward method in the `MultiHeadAttention` layer.\n",
      "\n",
      "So we could have something like this:\n",
      "\n",
      "```mermaid\n",
      "graph LR;\n",
      "    F([\"Embeddings ($$E \\in \\mathbb{R}^{b \\times l \\times d}$$)\"]) -->\n",
      "    FF([\"$$x_e = E_{[0,:,:]}$$\"])\n",
      "    FF --> G[\"Logistic Regression\"] --> H([$$y_0$$])\n",
      "```\n",
      "## Transformer architecture:\n",
      "\n",
      "<img src=\"figs/transformer_original.png\" />\n",
      "\n",
      "<img src=\"figs/transformer_annotated.png\" />\n",
      "## How far have we gone?\n",
      "\n",
      "So far, we have tokenized a batch of $b$ texts and have cropped/padded them to length $l$. After that, we calculated embeddings of dimension $d$ for our words. If everything went well, we now have embeddings $E \\in \\mathbb{R}^{b \\times l \\times d}$:\n",
      "\n",
      "```mermaid\n",
      "graph LR;\n",
      "    subgraph Tokenization;\n",
      "    Text([\"Texts (List of $$b$$ texts)\"]) --> A[\"Tokenizer\n",
      "        Cropping\n",
      "        Padding\"] --> B([\"Tokens (List, $$b \\times l$$)\"]) \n",
      "    end;\n",
      "\n",
      "    subgraph Embedding;\n",
      "    B --> C[\"Embedding\n",
      "    Layer\"] --> D([\"Embeddings ($$E \\in \\mathbb{R}^{b \\times l \\times d}$$)\"])\n",
      "    end;\n",
      "```\n",
      "\n",
      "However, if we are going to classify our text using Logistic Regression, we need a single vector to represent the whole sequence, that is, we need to convert:\n",
      "\n",
      "$$\n",
      "E \\in \\mathbb{R}^{b \\times l \\times d}\n",
      "$$\n",
      "\n",
      "to \n",
      "\n",
      "$$\n",
      "X \\in \\mathbb{R}^{b \\times d}\n",
      "$$\n",
      "\n",
      "In other words, we need to *summarize* the word-level embeddings into a single document-level embedding.\n",
      "\n",
      "The easiest way to do so is to calculate the mean of the embeddings, then we can proceed to classification:\n",
      "\n",
      "```mermaid\n",
      "graph LR;\n",
      "\n",
      "    subgraph Embedding;\n",
      "    D([\"Embeddings ($$E \\in \\mathbb{R}^{b \\times l \\times d}$$)\"])\n",
      "    end;\n",
      "\n",
      "    subgraph Summarization;\n",
      "    D --> E[\"Mean over time\"] --> F([\"Embeddings ($$X \\in \\mathbb{R}^{b \\times d}$$)\"])\n",
      "    end;\n",
      "\n",
      "    subgraph Classification;\n",
      "    F --> G[\"Logistic Regression\"] --> H([\"$$P(C_i=c_j | X_i)$$\"])\n",
      "    end;\n",
      "```\n",
      "\n",
      "Overall, we get 4-step architecture (tokenization, embedding, summarization, classification) that is, in fact, very similar to the one we had when we were using the vectorizer-classifier pipelines with the bag-of-words approach:\n",
      "\n",
      "```mermaid\n",
      "graph LR;\n",
      "\n",
      "    A([\"Text\"]) --> B[\"Tokenization\"];\n",
      "\n",
      "    subgraph Vectorization;\n",
      "    \n",
      "    B --> C[\"Embedding\"] --> D[\"Summarization\"];\n",
      "    end;\n",
      "    \n",
      "    subgraph Classification;\n",
      "    D --> E[\"Classification\"]\n",
      "    end;\n",
      " \n",
      "    E --> F([\"Prediction\"]);    \n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "The problem with this idea is that the calculation of the mean totally disregards the order of the words - essentially, we are doing a glorified bag-of-words modelling, which seems non-ideal. When we do so, we are We could find some other way to summarize our sequence of words so that we somehow account for the order of words.\n",
      "\n",
      "## Recurrent Neural Networks\n",
      "\n",
      "Recurrent Neural Networks (RNNs) were diffusely invented by many small contributions during the 1950s to the 1970s. The underlying idea is to begin with a simple logistic regression that receives as input the first time step of $E$, and yields a single prediction $y_0$: \n",
      "```mermaid\n",
      "graph LR;\n",
      "    F([\"Embeddings ($$E \\in \\mathbb{R}^{b \\times l \\times d}$$)\"])\n",
      "    FF([\"$$x_e = [E_{[t,:,:]}, y_{t-1}]$$\"])\n",
      "\n",
      "    F-->FF\n",
      "    FF --> G[\"Logistic Regression\"] \n",
      "\n",
      "    G --> H([$$y_t$$])\n",
      "\n",
      "    H -- \"Feedback loop\" --> FF\n",
      "```\n",
      "\n",
      "```mermaid\n",
      "graph LR;\n",
      "    subgraph Input;\n",
      "    T[\"Token embeddings\"];\n",
      "    P[\"Position embeddings\"];\n",
      "    S[\"Segment embeddings \n",
      "    (indicates if it is sentence 1\n",
      "     or sentence 2 in NSP task)\"];\n",
      "    ADD([\"\\+\"]);\n",
      "    T --> ADD;\n",
      "    P --> ADD;\n",
      "    S --> ADD; \n",
      "    end;\n",
      "\n",
      "    SEQ[\"Sequence Model\"];\n",
      "    ADD --> SEQ;\n",
      "    RES[\"Result: 1 vector per input token\"];\n",
      "    SEQ --> RES;\n",
      "```\n",
      "\n",
      "EXPECTED ANSWERS:\n",
      "\n",
      "1. On training, what is the model trying to predict? *the model predicts a probability distribution over all possible words so that the probability of the next word is maximized*\n",
      "1. What is the similarity between this architecture and a regular classifier? *the language model is simply a classifier in which the possible classes are all the possible words in a vocabulary*\n",
      "1. After training, why are some word embeddings so close to each other? *because they appear in similar situations in the training set*\n",
      "1. After the sequence model, what would happen to embeddings of phrases that use words that are close in the word embedding space? *they would be close to each other as well*\n",
      "1. If we use a logistic regression (LR) in a labeled dataset that links \"cats\" to a label \"positive\", but it has no mention of dogs, what would happen if we use this LR to predict the label for \"dogs\"? Why? *dogs and cats are close in the embedding space, hence by learning to classify cats we get the classification of dogs as a bonus*\n",
      "1. Common sense says: \"embeddings learn and capture the semantic relationships between words\". Rephrase this using more precise terms: what are embeddings really modelling, from a purely statistical/mathematical point of view? *embeddings are vector representations for words so that words that are close are more likely to be interchangeable to make possible phrases; however, this property is not observed for larger distances*\n",
      "1. Training the model with so few examples is already taking a significant amount of time. What could we do (as a society that works collaboratively) to mitigate this? *we could get a large machine and a huge dataset, train a language model, and share it so everyone can use their embeddings for their own purposes*\n",
      "\n",
      "```mermaid\n",
      "\n",
      "graph LR;\n",
      "    A((X)) --> B[\"$$\\times w_1^T$$\"];\n",
      "    subgraph Layer 1;\n",
      "    B --> C[\"\"$$+ b_1$$\"\"];\n",
      "\n",
      "    C --> D[\"\"\"f\"\"\"];\n",
      "        end;\n",
      "    subgraph Layer 2;\n",
      "    D --> E[\"$$\\times w_2^T$$\"];\n",
      "    E --> F[\"\"$$+ b_2$$\"\"];\n",
      "    end;\n",
      "    F --> G((ŷ))\n",
      "\n",
      "## MLP models\n",
      "\n",
      "A possible upgrade to the linear model is the MLP model. The MLP model is:\n",
      "\n",
      "$$\n",
      "\\hat{y} = f(xw_1^t+b_2)w_2^t+b_2,\n",
      "$$\n",
      "where $f$ is the Rectifying Linear Unit (ReLU) function given by $f(z)=0, z<0, f(z)=z, z>0$.\n",
      "\n",
      "We can interpret this equation as two layers of linear projections, separated by a non-linear operation, as follows:\n",
      "    \n",
      "Note that we defined the size of the weight matrix using the `in_features` and `out_features` in our linear layer. The number `in_features` is the dimension of the input (probably our $d$), and `out_features` allows us to calculate several $z_j$ vectors simultaneously and independently. For example, having `in_features=2` and `out_features=3` leads to:\n",
      "\n",
      "$$\n",
      "\\begin{bmatrix}\n",
      "z_{1,1} & z_{1,2} & z_{1,3} \\\\\n",
      "z_{2,1} & z_{2,2} & z_{2,3} \\\\\n",
      "\\cdots \\\\\n",
      "z_{N, 1} & z_{N,2} & z_{N,3} \\end{bmatrix} = \\beta_0 + \\begin{bmatrix} \n",
      "                        x_{1,1} & x_{1,2} & \\cdots & x_{1,d} \\\\\n",
      "                        x_{2,1} & x_{2,2} & \\cdots & x_{2,d} \\\\\n",
      "                        \\cdots & \\cdots & \\cdots & \\cdots \\\\\n",
      "                        x_{N,1} & x_{N,2} & \\cdots & x_{N,d} \\\\\n",
      "                        \\end{bmatrix}\n",
      "                        \\begin{bmatrix}\n",
      "                        \\beta_{1,1} & \\beta_{1,2} & \\beta{1,3}\\\\\n",
      "                        \\beta_{2,1} & \\beta_{2,2} & \\beta{2,3}\\\\\n",
      "                        \\cdots \\\\\n",
      "                        \\beta_{d,1} & \\beta_{d,2} & \\beta_{d,3}\n",
      "                        \\end{bmatrix}.\n",
      "$$\n",
      "\n",
      "### Residual blocks\n",
      "\n",
      "The problem of zero-gradient has been tackled by many approaches. One of the most successfull was to create an alternate route for gradients to propagate. This route is called \"residual\", and involves adding the input to the output of the network, that is:\n",
      "\n",
      "```mermaid\n",
      "\n",
      "graph LR;\n",
      "    A((X)) --> B[\"$$\\times w_1^T$$\"];\n",
      "    subgraph Layer 1;\n",
      "    B --> C[\"\"$$+ b_1$$\"\"];\n",
      "\n",
      "    C --> D[\"\"\"f\"\"\"];\n",
      "        end;\n",
      "    subgraph Layer 2;\n",
      "    D --> E[\"$$\\times w_2^T$$\"];\n",
      "    E --> F[\"\"$$+ b_2$$\"\"];\n",
      "    end;\n",
      "    F --> H(\"SUM\");\n",
      "    A -->|Residual Connection| H;\n",
      "    H --> G((ŷ));\n",
      "```\n",
      "$$\n",
      "\\hat{y} = x + (f(xw_1^t+b_2)w_2^t+b_2),\n",
      "$$\n",
      "## Exercise: What happens to embeddings during training?\n",
      "\n",
      "We will now put everything to use and make some scientific experiments. Feel free to work in groups.\n",
      "\n",
      "The question is: **what happens to embeddings while we train our network**?\n",
      "\n",
      "Follow the steps below to find out.\n",
      "\n",
      "1. Add a method in `ClassifierWithRNN` that receives a token sequence as input (similarly to forward) but returns the result of the summarization.\n",
      "2. Download the FakeNewsNet dataset from Kagglehub (code is below)\n",
      "3. Make a `ClassifierWithRNN` with a hidden dimension of $2$. \n",
      "4. Calculate document-level embeddings for the elements of the dataset *before* training and show them in a scatter plot. Use colors to represent the classes of each item.\n",
      "5. Train your classifier.\n",
      "6. Calculate document-level embeddings for the elements of the dataset *after* training and show them in a scatter plot. Use colors to represent the classes of each item.\n",
      "\n",
      "What could you observe? Why do you think that is happening?\n",
      "\n",
      "Now, do it again using the summarization based on mean. What do you observe? Why is that happening?\n",
      "```mermaid\n",
      "graph LR;\n",
      "\n",
      "    subgraph Embedding;\n",
      "    D([\"Embeddings ($$E \\in \\mathbb{R}^{b \\times l \\times d}$$)\"])\n",
      "    end;\n",
      "\n",
      "    subgraph Summarization;\n",
      "    D-->FF([\"$$x_e = [E_{[t,:,:]}, y_{t-1}]$$\"])\n",
      "\n",
      "    FF --> GF[\"$$\\tanh(xw^T+b)$$\"] \n",
      "\n",
      "    GF --> HF([$$y_t$$])\n",
      "\n",
      "    HF -- \"Feedback loop\" --> FF\n",
      "    HF -- \"Last\n",
      "    State\" --> F([\"Embeddings ($$X \\in \\mathbb{R}^{b \\times d_h}$$)\"])\n",
      "    end;\n",
      "\n",
      "    subgraph Classification;\n",
      "    F --> G[\"Logistic Regression\"] --> H([\"$$P(C_i=c_j | X_i)$$\"])\n",
      "    end;\n",
      "```\n",
      "## PART 2: action\n",
      "\n",
      "(a) Make a classifier that uses BERT embeddings to categorize the texts in the dataset we have discussed.\n",
      "\n",
      "(b) Make a bar plot comparing the accuracy of the BERT-based classifier to that of the Bag-of-Words classifier\n",
      "\n",
      "(c) Use a PCA or a T-SNE plot to visualize the documents in the newsgroups dataset in the embedding space provided by BERT. Analyze the plot taking into account the confusion matrix or the classification report of your BERT-based classifier.\n",
      "\n",
      "\n",
      "\n",
      "## Some steps on optimization\n",
      "\n",
      "You may have noticed that we are using the MSE loss in the code above. This is definitely not optimal, because reductions in MSE+ does not necessarily correspond to increases in accuracy. Instead, we could use the cross-entropy loss.\n",
      "\n",
      "### Cross-entropy loss\n",
      "\n",
      "The idea of the cross-entropy loss begins with the negative log likelihood (NLL). Is is usually written as:\n",
      "\n",
      "$$\n",
      "\\text{NLL} = - \\sum_i \\log{P(C_i = c_i|X_i)},\n",
      "$$\n",
      "\n",
      "where $C_i=c_i$ is the event that the predicted class $C_i$ for the $i$-th input $X_i$ is the correct class $c_i$.\n",
      "\n",
      "Note that NLL measures the likelihoood that the correct class will be predicted for each item $i$ in the dataset. Therefore, maximizing the negative log likelihood means minimizing the probability that a wrong class will be predicted. Importantly, this is *very* different from minimizing the MSE!\n",
      "\n",
      "We know that $P(C_i = c_i|X_i)$, in the logistic regressor, is the result of applying the sigmoid (logistic) function to the logits $z_i$. I will call $y_i = P(C_i = c_i|X_i)$ to make reading easier:\n",
      "\n",
      "$$\n",
      "y_i = \\sigma(z_i) = \\frac{1}{1+e^{-z_i}}\n",
      "$$\n",
      "\n",
      "\n",
      "\n",
      "We can substitute $y_i$ in the NLL equation to get:\n",
      "\n",
      "$$\n",
      "\\text{NLL} = - \\sum_i \\log{y_i} = -\\sum_i \\log{\\frac{1}{1+e^{-z_i}}},\n",
      "$$\n",
      "\n",
      "Remember that the log of a division is the subtraction of logs. Hence, we get:\n",
      "\n",
      "$$\n",
      "\\text{NLL} = -\\sum_i \\log{1} - \\log{1+e^{-z_i}} = -\\sum_i 0 - \\log{1+e^{-z_i}} = \\sum_i \\log{(1+e^{-z_i})}.\n",
      "$$\n",
      "\n",
      "Also, we are using SGD as the optimizer. The SGD optimizer uses a fixed-step optimization over the parameter space. A more modern approach is the Adam (Adaptive Momentum) optimizer, which adapts the step size so that smoother regions of the parameter space are swept more quickly, and rougher regions automatically lead to smaller steps.\n",
      "\n",
      "Then, the code becomes this:\n",
      "    \n",
      "Note that we defined the size of the weight matrix using the `in_features` and `out_features` in our linear layer. The number `in_features` is the dimension of the input (probably our $d$), and `out_features` allows us to calculate several $z_j$ vectors simultaneously and independently. For example, having `in_features=2` and `out_features=3` leads to:\n",
      "\n",
      "$$\n",
      "\\begin{bmatrix}\n",
      "z_{1,1} & z_{1,2} & z_{1,3} \\\\\n",
      "z_{2,1} & z_{2,2} & z_{2,3} \\\\\n",
      "\\cdots \\\\\n",
      "z_{N, 1} & z_{N,2} & z_{N,3} \\end{bmatrix} = \\beta_0 + \\begin{bmatrix} \n",
      "                        x_{1,1} & x_{1,2} & \\cdots & x_{1,d} \\\\\n",
      "                        x_{2,1} & x_{2,2} & \\cdots & x_{2,d} \\\\\n",
      "                        \\cdots & \\cdots & \\cdots & \\cdots \\\\\n",
      "                        x_{N,1} & x_{N,2} & \\cdots & x_{N,d} \\\\\n",
      "                        \\end{bmatrix}\n",
      "                        \\begin{bmatrix}\n",
      "                        \\beta_{1,1} & \\beta_{1,2} & \\beta{1,3}\\\\\n",
      "                        \\beta_{2,1} & \\beta_{2,2} & \\beta{2,3}\\\\\n",
      "                        \\cdots \\\\\n",
      "                        \\beta_{d,1} & \\beta_{d,2} & \\beta_{d,3}\n",
      "                        \\end{bmatrix}.\n",
      "$$\n",
      "\n",
      "or, more compactly:\n",
      "\n",
      "$$\n",
      "z_{N \\times J} = x_{N \\times d} w^T_{d \\times j} + b_{1 \\times j}\n",
      "$$\n",
      "\n",
      "Note that $w \\in \\mathbb{R}^{j \\times d}$, hence $w^T \\in \\mathbb{R}^{d \\times j}$.\n",
      "\n",
      "## Multiclass Logistic Regression\n",
      "\n",
      "Although sklearn deals with multiclass classification automatically, in PyTorch we have to take care of some things:\n",
      "\n",
      "1. The number of outputs of the classification layer must be equal to the number of classes; and\n",
      "1. Instead of using a sigmoid, we will be using a SoftMax activation function.\n",
      "1. Use the Cross Entropy loss instead of the Binary Cross Entropy loss.\n",
      "\n",
      "How does that work?\n",
      "\n",
      "First, let's look at the number of outputs.\n",
      "\n",
      "In the binary classifier, we had the equation:\n",
      "\n",
      "$$\n",
      "\\boldsymbol{z} = \\boldsymbol{x}\\boldsymbol{w^T} + b\n",
      "$$\n",
      "\n",
      "and this was written as:\n",
      "\n",
      "$$\n",
      "\\boldsymbol{z} = \\begin{bmatrix} x_1 & x_2 & \\cdots x_d \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_d \\end{bmatrix} + b\n",
      "$$\n",
      "\n",
      "### Adding more outputs to our logistic regression\n",
      "\n",
      "Now, we are going to add another line of weights in $\\boldsymbol{w}$, which actually means we will have another columns of weights in $\\boldsymbol{w^T}$:\n",
      "\n",
      "$$\n",
      "\\boldsymbol{z} = \\begin{bmatrix} x_1 & x_2 & \\cdots x_d \\end{bmatrix} \\begin{bmatrix} w_{1,1} & w_{1,2} \\\\ w_{2,1} & w_{2,2} \\\\ \\vdots  & \\vdots \\\\ w_{d,1} & w_{d,2} \\end{bmatrix} + \\begin{bmatrix}b_1& b_2\\end{bmatrix}\n",
      "$$\n",
      "\n",
      "Thus, $\\boldsymbol{z} \\in \\mathbb{R}^{1 \\times 2}$, that is, there are *two* elements of $\\boldsymbol{z}$ for each input $\\boldsymbol{x}$.\n",
      "\n",
      "We can further expand this idea and add many rows to $\\boldsymbol{w}$, up to $c$ rows, which leads to:\n",
      "\n",
      "$$\n",
      "\\boldsymbol{z} = \\begin{bmatrix} x_1 & x_2 & \\cdots x_d \\end{bmatrix} \\begin{bmatrix} w_{1,1} & w_{1,2} & \\cdots & w_{1,c} \\\\ w_{2,1} & w_{2,2} & \\cdots & w_{2,c} \\\\ \\vdots  & \\vdots & \\ddots & \\vdots \\\\ w_{d,1} & w_{d,2} & \\cdots & w_{d,c} \\end{bmatrix} + \\begin{bmatrix}b_1& b_2 & \\cdots & b_c\\end{bmatrix}\n",
      "$$\n",
      "\n",
      "Now, our logit $\\boldsymbol{z}$ has as many as $c$ dimensions. Let's make $c$ the number of classes we have in our problem. Now, we obtain the output $\\boldsymbol{y}$ by normalizing $\\boldsymbol{z}$ using a specific function.\n",
      "\n",
      "In our problem, we are assuming that each document can have only one of a set of $c$ known classes. Hence, our output must me a probability distribution of classes, conditioned to the input, or:\n",
      "\n",
      "$$\n",
      "y_i = P(C = c_i | \\boldsymbol{x})\n",
      "$$\n",
      "\n",
      "Such a normalization can be obtained using:\n",
      "\n",
      "$$\n",
      "\\boldsymbol{y} = \\text{Softmax}(\\boldsymbol{z}) = \\frac{e^{z_i}}{\\sum_{i=1}^c e^{z_{i}}}\n",
      "$$\n",
      "\n",
      "In other words: calculate the exponential of all logits, and then normalize them by the sum of all logit exponentials.\n",
      "\n",
      "Softmax is important because, by applying it, the outputs become interpretable as a probability distribution over the possible classes in the output. The exponential is important because it accounts for uncertainty: logits with higher certainty are typically more extreme, and lead to distributions with lower entropy.\n",
      "\n",
      "Last, we use Cross Entropy loss because the distribution is no longer 0-1. Instead, the cross entropy is calculated along all dimensions of the output.\n",
      "\n",
      "### Exercise: evaluate a model\n",
      "\n",
      "Models supporting many classes tend to have more parameters, hence it becomes progressively more difficult to train them adequately. Because of that, this exercise will start with pre-trained weights. See - we do everything the same way as before. However, we must:\n",
      "\n",
      "1. Define the model object **exactly as it was trained**\n",
      "1. Load the weights from a state dictionary.\n",
      "\n",
      "PyTorch forbids saving and loading via joblib because that could allow executing malicious code in the machine that loads the models.\n",
      "## Exercise: use pre-trained weights from a different architecture\n",
      "\n",
      "Check the file `train_newsgroups_rnn_minibatch.py`. It has code to train an RNN-based classifier over the newsgroups dataset. Don't run it - it will take a while. Instead, open it, and find:\n",
      "\n",
      "1. The model definition\n",
      "2. The file where the model weights are\n",
      "\n",
      "Instantiate a model using the neural network trained in `train_newsgroups_rnn_minibatch.py` and evaluate its accuracy and visualize its embeddings.\n",
      "\n",
      "# Multi-Head Attention Encoders\n",
      "\n",
      "If you were not living under a rock in the last few years, you probably heard about something called a \"transformer\". This is a neural network topology made famous in an article called [Attention is All you Need](https://arxiv.org/abs/1706.03762) (which revolutionized both how we model neural networks for NLP, and how we give titles to our papers on the topic). This paper discusses many ingenious mechanisms, some of which we will discuss now.\n",
      "\n",
      "Remember that in LSTMs we have the Forget Gate (if you have... ahem... *forgotten* about it, go back and read again!)? The Forget Gate is actually saying that information from the past $c_{t-1}$ should be weighted based on some criteria - and LSTM uses the output of an auxiliary MLP to find this weight. This means that the network memory has a relevance that is defined by the behavior of its surroundings.\n",
      "\n",
      "## The base idea of attention\n",
      "\n",
      "In the [Attention paper, Vaswani and their colleagues](https://arxiv.org/abs/1706.03762) observed that this is (at least in a metaphorical sense) similar to calculate a weighted average of the inputs, where weights represent the relevance of each input. Also, they observed that recurrent networks (even LSTMs) tend to give more relevance to words that are close by, even if they can potentially draw information from long distances - and this is not something desireable. Hence, they came with a clever solution.\n",
      "\n",
      "Their solution starts with calculating the pairwise similarity between each item in the input (or: between each word embedding). For such, they use the inner product between each word, or simply:\n",
      "\n",
      "$$\n",
      "S = X X^T\n",
      "$$\n",
      "\n",
      ", where $X \\in \\mathbb{R}^{T \\times D}$ contains $T$ word embeddings with dimension $D$.\n",
      "\n",
      "The result $S \\in \\mathbb{R}^{T \\times T}$ indicates the similarity between each pair of words in the sequence.\n",
      "\n",
      "After that, $S$ is divided by $\\sqrt{D}$ as a form of normalization, that is, this avoids finding larger values if the embedding dimension changes.\n",
      "\n",
      "Finally, they normalize each line of $S$ using a softmax function, which transforms each line of $S$ into a probability distribution (that is, it sums to $1$).\n",
      "\n",
      "At the end of this process, they have a weight matrix (they don't actually call it a weight matrix, but I am doing so):\n",
      "\n",
      "$$\n",
      "W = \\text{softmax} (XX^T/\\sqrt{D})\n",
      "$$\n",
      "\n",
      "At this point, each element $w_{t1,t2}$ of $W$ contains the weight given to token $t2$ at time $t1$. To apply these weights, they multiply $W$ by $X$, so that:\n",
      "\n",
      "$$\n",
      "Y = WX = \\text{softmax} (XX^T/\\sqrt{D})X.\n",
      "$$\n",
      "\n",
      "Thus, $Y \\in \\mathbb{R}^{T \\times D}$ contains a transformation of the sequence (this is not stated, but I guess the name \"transformer\" comes from this idea) where each output is a weighted mean of the inputs. The weights can be seen as the \"relevance\" of each input, that is, points to which the system should pay *attention* - henceforth, *attention is all you need*.\n",
      "\n",
      "## Query, key, value\n",
      "\n",
      "The attention mechanism was observed to be similar to a search with a query in in a key-value database. In this type of database, we give relevance to values based on the similarity between a query and a key. Because of that, the output is actually calculated using three separated inputs, as in:\n",
      "\n",
      "$$\n",
      "Y = \\text{softmax} (QK^T/\\sqrt{D})V.\n",
      "$$\n",
      "\n",
      "An important step here is to allow three different inputs, instead of forcing $Q=K=V$. Thus, we have inputs $X_q, X_k, X_v$. Also, the paper proposed making linear projections of the inputs to obtain $Q,K,V$, that is:\n",
      "\n",
      "$$\n",
      "Q = X_qW_q^T\\\\\n",
      "K = X_kW_k^T\\\\\n",
      "V = X_vW_v^T,\n",
      "$$\n",
      "\n",
      "and this allows learning the weight matrices $W_q, W_k, W_v$.\n",
      "\n",
      "## Positional encoding\n",
      "\n",
      "It was also observed that the weighted mean in the attention mechanism [disconsiders the order of the words](https://arxiv.org/abs/1706.03762), which is something we were thriving for when using the LSTM. The proposed solution for this is to use an auxiliary sequence of tokens that represents the position of each word in the sequence. Then, each of these \"position tokens\" receives an embedding (this could be a trained embedding, although the transformer paper uses a [pre-defined cosine-sine embedding](https://arxiv.org/abs/1706.03762) - they both seem to work fine). This position encodings are added to the word embeddings and the result of this operation is, itself, propagated as the input of the network, as in:\n",
      "\n",
      "<img src=\"figs/positional_encoding.png\" />\n",
      "\n",
      "## Multi-head attention and Pytorch implementation\n",
      "\n",
      "It was observed that using multiple attention mechanisms in parallel can improve final results. Thus, the idea is to yield the same input to multiple attention mechanisms, and then cocatenate the results. This is somewhat ingenious, at it allows giving attention to different parts of the input for different reasons.\n",
      "\n",
      "The pytorch [API for a multihead attention layer](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html) is very simple:\n",
      "\n",
      "    mhe = nn.MultiheadAttention(embed_dim, num_heads)\n",
      "\n",
      "where `embed_dim` must be divisible by `num_heads`.\n",
      "\n",
      "### Summarization for classification\n",
      "\n",
      "You have probably noted that the output of a multi-head attention layer yields outputs for each time step in the input. We still need to summarize it. However, this time, we can use the *timewise mean* to summarize. This idea has been used in the [keras implementation of a multihead-attention classifier](https://keras.io/examples/nlp/text_classification_with_transformer/), and it draws from the fact that we have already considered the word positions in our representation when we used positional encoders.\n",
      "\n",
      "## A linear layer is a linear predictor?\n",
      "\n",
      "Allright, so the linear layer works essentially the same as the logistic regression. Let's show it!\n",
      "\n",
      "First, get back to the classifier we made in the beginning of this class. If you haven't fitted it yet, do it now.\n",
      "\n",
      "In the code below, we get the weights from the fitted logistic regression and substitute them in our linear layer:\n",
      "\n",
      "## Exercise: analyze a dataset\n",
      "\n",
      "The dataset defined below receives a sequence of tokens as input.\n",
      "\n",
      "1. When it is used to train a classifier, what are the inputs, and what is the \"output class\"?\n",
      "1. If a classifier using this dataset estimates $P(y | x)$, what is $y$ and what is $x$ in the real world? \n",
      "## Multiclass classifiers and Language Models\n",
      "\n",
      "**GOAL: At the end of this class, we will be able to interpret and discuss embeddings according to their mathematical formulation.**\n",
      "\n",
      "So far, we have been dealing with binary classifiers. However, many classification problems are multiclass, that is, there are many classes we can be choosing from. For example, the `20newsgroups` dataset has 20 different classes, and the objective is to classify items among these 20 classes. We can access the dataset using:\n",
      "### Normalization\n",
      "\n",
      "The non-linearities allow applying different transforms to each region of the input space. The residual connections avoids the vanishing gradient problem. Now, we add an extra layer of stability by normalizing data in each layer. Normalization helps maintaining all representations within reasonable values, which helps numerical stability and has ultimately been linked to faster convergence in neural networks.\n",
      "\n",
      "Using normalization after each layer, we do observe a faster convergence:\n",
      "When we try to approximate this using a linear layer, we obviously can't. This is due to our data being more complicated than the model - or, in other words, our model is not *expressive* enough to model our data. In the animation, we clearly see that the linear layer can only apply the same transform to all points in the input vector space, hence they all \"bend\" in the same way.\n",
      "\n",
      "Our model is unable, for example, to model the different cluster variances generated by the different multiplications applied when we generated each part of $y$.\n",
      "# End-to-end Neural Networks for NLP\n",
      "\n",
      "## Word Embeddings\n",
      "\n",
      "If everything is going well, you have used PyTorch to make a classifier based on logistic regression. Also, you are probably mildly annoyed by the fact that the accuracy was essentially the same as what we had with our previous version using scikit-learn, with the added difficulty that we had to write a loop all by ourselves.\n",
      "\n",
      "Now it is time to do some math and review our models.\n",
      "\n",
      "### Some underlying math of the bag-of-words model\n",
      "\n",
      "In our current model, we represent each word by a line-vector $x_n$ with $v$ dimensions, where $v$ is the length of our vocabulary. Therefore, a document with $n$ words can be represented by a matrix of words within a document $X^{(d)}$ as:\n",
      "\n",
      "$$\n",
      "X^{(d)} = \n",
      "\\begin{bmatrix} \n",
      "                        x^{(d)} _{1,1} & x^{(d)}_{1,2} & \\cdots & x^{(d)}_{1,d} \\\\\n",
      "                        x^{(d)}_{2,1} & x^{(d)}_{2,2} & \\cdots & x^{(d)}_{2,d} \\\\\n",
      "                        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
      "                        x^{(d)}_{N,1} & x^{(d)}_{N,2} & \\cdots & x^{(d)}_{N,d} \\\\\n",
      "                        \\end{bmatrix}\n",
      "$$\n",
      "\n",
      "Then, we essentially sum all elements of the matrix to get a count. This maps the sequence of words to a single vector $x$ that represents our document:\n",
      "\n",
      "$$\n",
      "x^\\prime_j = \\sum _{i=1}^n x^{(d)}_{i,j}\n",
      "$$\n",
      "\n",
      "This is the same as pre-multiplying $W$ by a line-matrix of ones:\n",
      "\n",
      "$$\n",
      "x^\\prime_j = [1, 1, \\cdots, 1] \\begin{bmatrix}\n",
      "x^{(d)}_1 \\\\\n",
      "x^{(d)}_2 \\\\ \n",
      "\\cdots \\\\\n",
      "x^{(d)}_n\n",
      "\\end{bmatrix}\n",
      "$$\n",
      "\n",
      "Last, if we want to binarize this vector, we simply apply an element-wise non-linearity such as:\n",
      "\n",
      "$$\n",
      "x = f(x^\\prime),\n",
      "$$\n",
      "where $f(x)=1$ if $x>0$ and $f(x)=0$ otherwise.\n",
      "\n",
      "Essentially, what we must do is:\n",
      "\n",
      "1. Get a representation for each word\n",
      "1. Combine the word representations for a document into a single representation\n",
      "1. Use some non-linearity to modulate the representation as needed\n",
      "\n",
      "Our solutions, so far, are:\n",
      "\n",
      "1. Use a one-hot encoding\n",
      "1. Use a simple summation\n",
      "1. Use a step-function with a suitable threshold\n",
      "\n",
      "The reason we have these solutions is that they somehow *make sense*. But now let's raise a question: are they the *best* solutions?\n",
      "\n",
      "### Problems with the sparse representation\n",
      "\n",
      "The one-hot encoding for words is easy to understand, but, at the same time, has some problems. \n",
      "\n",
      "The first, and more obvious, is that the dimensionality of the representation grows when the vocabulary grows. This is a problem because the amount of required data to train a system is typically a function of its dimension - hence, the more words we have, the more data we need. The vocabulary typically grows when we add more texts to the dataset, so this actually means: the more data we have, the more data we need.\n",
      "\n",
      "The second is that our data matrix starts getting too big. Maybe it stops fitting memory - and this is because we have so many columns! Maybe we could do something about this?\n",
      "\n",
      "The third, and less obvious, is that the distance between words is the same, regardless of their meaning. Hence, when we learn about dungeons, our system learns nothing about dragons. In fact, classifiers based on one-hot bag-of-words have no internal representation indicating that dungeons and dragons go more or less together.\n",
      "\n",
      "### Dense representations\n",
      "\n",
      "So, let's do something else. Let's say that each word is now going to be represented as a vector in some $\\mathbb{R}^N$ space. We are free to choose $N$. This representation is called *embedding*.\n",
      "\n",
      "Embeddings are *dense* representations, meaning that all dimensions are important. The name *dense* opposes to the *sparse* idea of the one-hot encoding in which only one dimension is important per word, and only a few dimensions are important per document.\n",
      "\n",
      "What we *want* here is to have all words being represented in locations of $\\mathbb{R}^N$ that reflect their meaning. But, what is *meaning*?\n",
      "\n",
      "This is a complicated question, which we will approach during this course.\n",
      "\n",
      "For now, let's go on and code a little.\n",
      "\n",
      "### Coding with embeddings\n",
      "\n",
      "To map words to their embeddings, we use a data structure that is very similar to a dictionary. The structure is wrapped in an *embedding layer* in Pytorch. \n",
      "\n",
      "It receives a sequence of integers as input. Each of these integer corresponds to a token in the vocabulary. For now, we will assume that each token corresponds to a word. Then, it yields a sequence of vectors for each document:\n",
      "## A simple dataset: a rotation + translation\n",
      "\n",
      "A linear regression is capable to find the correct rotation and translation of a dataset. This is because rotations and translations can be immediately expressed by the linear prediction equation $y = xw^t + b$ - in this case, the weight matrix $w$ can be constructed from a rotation, and $b$ corresponnds to the translation:\n",
      "# Modelling sequences with RNNs\n",
      "\n",
      "When we go from sequences of word embeddings to a document-wise vector representation that can be classified, we have to somehow summarize a sequence of vectors into a single vector. So far, what we have been doing is:\n",
      "\n",
      "1. Get one embedding $e \\in \\mathbb{R}^{l \\times d}$ per token, where  $l$ is the sequence length and $d$ is the embedding dimension. This generates the embedding matrix $E \\in \\mathbb{R}^{b \\times l \\times d}$, where $b$ is the batch size.\n",
      "2. Calculate the timewise mean of $E$, generating $X \\in \\mathbb{R}^{b \\times d}$\n",
      "3. Proceed to classification with our logist regression.\n",
      "\n",
      "This is something like this:\n",
      "In the animation, we see that each part of the space is being folded and bended differenty. This is because of the non-linearity given by the ReLU function.\n",
      "\n",
      "### A very small example\n",
      "\n",
      "Let's suppose we have two 1-dimensional inputs: $x_1$ and $-x_2$, where $x_1$ and $x_2$ are real and positive. Our network has 1-d outputs as well.\n",
      "\n",
      "For simplicity, let's assume $b_1=b_2=0$\n",
      "\n",
      "$w_1$ will be equal to $\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$. Thus, $xw_1^T=\\begin{bmatrix} x_1 & -x_2 \\\\ -x_1 & x_2 \\end{bmatrix}$.\n",
      "\n",
      "Now, after applying $f(.)$ to $xw_1^T$, we have:\n",
      "\n",
      "$$\n",
      "f(xw_1^T)=\\begin{bmatrix} x_1 & 0 \\\\ 0 & x_2 \\end{bmatrix}\n",
      "$$\n",
      "\n",
      "This is important: each one of our rows are independent of each input!\n",
      "\n",
      "Now, note that $w_2$ must be 2x1 (let's say it is equal to $[c, d]$), and:\n",
      "\n",
      "$$\n",
      "y = \\begin{bmatrix} x_1 & 0 \\\\ 0 & x_2 \\end{bmatrix} \\begin{bmatrix} c \\\\ d \\end{bmatrix} = \\begin{bmatrix} c x_1 \\\\ d x_2 \\end{bmatrix} \n",
      "$$\n",
      "\n",
      "Now, importantly: in our model, the first input received a scale of $c$, while the second input received a scale of $d$.\n",
      "\n",
      "Thus, the model operates in two layers. In the first layer, it divides the inputs into groups; in the second layer, it applies a different linear transform for each group.\n",
      "\n",
      "### Drawbacks\n",
      "\n",
      "The values for the weight and bias matrices must be trained using gradient descent. However, $f(.)$ has zero gradient for all negative inputs. For this reason, it is common to see output values organizing in straight lines - located exactly where the point of inflection is.\n",
      "Now, let's recall how Logistic Regression works!\n",
      "\n",
      "The fitted predictor expects an input with $d$ features.\n",
      "\n",
      "We use the `vectorizer` to map each text in `X_test` to a vector with $d$ elements. Each of these vectors is a line $[x_1, x_2, x_3 \\cdots x_d]$ in `X_vect` (note that the index $d$ in $x_d$ is the same as the number of features expected by the fitted predictor.\n",
      "\n",
      "Then, Logistic Regression uses its fitter weights $\\beta$ to calculate a weighted sum of the elements in the input, that is:\n",
      "\n",
      "$$\n",
      "z = \\beta_0 + x_1 \\beta_1 + x_2 \\beta_2 + \\cdots + x_d \\beta_d.\n",
      "$$\n",
      "\n",
      "Note that if we have a matrix made of $N$ lines of features (each line corresponding to a dataset item!), then we can calculate the output for each line $i$ using:\n",
      "\n",
      "$$\n",
      "z_i = \\beta_0 + x_{i,1} \\beta_1 + x_{i,2} \\beta_2 + \\cdots + x_{i,d} \\beta_d.\n",
      "$$\n",
      "\n",
      "This can be translated into a matrix multiplication:\n",
      "\n",
      "$$\n",
      "\\begin{bmatrix}\n",
      "z_1 \\\\\n",
      "z_2 \\\\\n",
      "\\cdots \\\\\n",
      "z_N \\end{bmatrix} = \\beta_0 + \\begin{bmatrix} \n",
      "                        x_{1,1} & x_{1,2} & \\cdots & x_{1,d} \\\\\n",
      "                        x_{2,1} & x_{2,2} & \\cdots & x_{2,d} \\\\\n",
      "                        \\cdots & \\cdots & \\cdots & \\cdots \\\\\n",
      "                        x_{N,1} & x_{N,2} & \\cdots & x_{N,d} \\\\\n",
      "                        \\end{bmatrix}\n",
      "                        \\begin{bmatrix}\n",
      "                        \\beta_1 \\\\\n",
      "                        \\beta_2 \\\\\n",
      "                        \\cdots \\\\\n",
      "                        \\beta_d\n",
      "                        \\end{bmatrix}.\n",
      "$$\n",
      "\n",
      "We can simplify this by defining a weight matrix $w = [\\beta_1, \\beta_2, \\cdots, \\beta_d]$, and an input matrix $X$ containing all elements $x_{i,j}$ so that:\n",
      "\n",
      "$$\n",
      "z = \\beta_0 + X w^T\n",
      "$$\n",
      "\n",
      "Last, we apply the logistic function to each element of the vector $z$, and then we have a prediction.\n",
      "\n",
      "The elements of $z$ are called *logits*, $\\beta_0$ is called *bias* and the elements of $w$ are called *weights*.\n",
      "\n",
      "Good, but we didn't come this far to hear again about Logistic Regression. We now shall proceed!\n",
      "\n",
      "\n",
      "The output $y$ is often called \"hidden state\" and referred to as $h$. This is a reference to the fact that this output is usually a part of a larger classifier, and is hidden because it is an intermediate result of the network (similarly to the intermediate results in a deep MLP).\n",
      "\n",
      "\n",
      "\n",
      "Typically, RNNs are used to summarize sequences by propagating the last output, that is, our summarization of a sequence of \n",
      " elements is simply $X=y_l$.\n",
      "\n",
      "Now, note that we have no restriction as to what the dimension of $y$ should be. We could choose to yield, for example, $y_t$ with 50 dimensions, or 500 dimensions. This choice would simply imply in greater dimensions for $x$ and more degrees of freedom in the logistic regression. If this dimension is $d_e$, then $X \\in \\mathbb{R}^{b \\times d_e}.$\n",
      "\n",
      "Hence, our classifier now works like this:\n",
      "## How to train a Logistic Regression\n",
      "\n",
      "Remember that our classifier outputs:\n",
      "\n",
      "\n",
      "$$\n",
      "\\hat{y} = \\sigma(z) = \\sigma(Xw^T + b),\n",
      "$$\n",
      "where $\\sigma()$ denotes the sigmoid (or logistic) function?\n",
      "\n",
      "The procedure to adjust the weights and biases in our linear layer is to use actual examples of outputs ($y$) and compare them to our estimate $\\hat{y}$. They are probably not going to be the same, so we can calculate how $y$ and $\\hat{y}$ are different using a loss function $L(y, \\hat{y})$. Then, we are going to calculate the derivative of $L$ with respect to all weights and biases, that is, we will have:\n",
      "\n",
      "$$\n",
      "g_i = \\frac{d L}{d p_i}\n",
      "$$\n",
      "\n",
      "for each parameter $p_i$ in the linear layer (either a weight or a bias). The parameters in the linear layer are the weights and biases. \n",
      "\n",
      "You might have noticed that $g$ is a vector of derivatives. Actually, you might even remember that it is called a *gradient* vector written as $\\nabla L$.\n",
      "\n",
      "Remember that the gradient points towards the direction in which $L$ grows the most if we change the parameters $p$? So, let's do the opposite! We want to decrease our loss, so we will change our parameters by *subtracting* a little bit of the gradient vector, that is, we iteractively update the parameters as:\n",
      "\n",
      "$$\n",
      "p_i \\leftarrow \\alpha \\frac{\\partial L}{\\partial p_i},\n",
      "$$\n",
      "\n",
      "where $\\alpha$ is a small value of our choice. A lower $\\alpha$ means steps will be smaller, which can make it take longer to converge, whereas a higher value can lead to instabilities.\n",
      "\n",
      "We could be fancier and write this in vector form:\n",
      "\n",
      "$$\n",
      "p \\leftarrow \\alpha \\nabla_{\\mathbf{p}} L.\n",
      "$$\n",
      "\n",
      "What we usually do is to calculate the gradient $g$ for each item in the training data, and then sum (or average?) it before applying the step to the parameters. This process of going through the whole dataset is called *epoch*.\n",
      "\n",
      "Now, how do we implement this? Let's go!\n",
      "\n",
      "Now, remember that logistic regression works by using $y=\\sigma(z)$, where $z = xw^t+b$. We can change the non-linear function for this purpose - usually, the applied function is $\\tanh(z)$. The hyperbolic tangent $\\tanh$ can be calculated by:\n",
      "\n",
      "$$\n",
      "\\tanh(z) = 2\\sigma(z)-1.\n",
      "$$\n",
      "\n",
      "The advantage of $\\tanh$ is that it can assume negative values, which can be useful for optimization. However, when we use $\\tanh$ instead of $\\sigma$, we no longer have a logistic regression. Instead, we have a linear layer followed by a non-linear function, that is:\n",
      "\n",
      "## Zero-padding and truncation: keeping sentences the same length\n",
      "\n",
      "If you have been paying attention, you probably realized that, in any language, sentences have different lengths. However, PyTorch works with tensors, hence the inputs for an embedding layer should have the same length.\n",
      "\n",
      "If we find long sentences, we could just truncate them so a desired length.\n",
      "\n",
      "However, if we find a short sentence, the procedure is different. Usually, we inser a special token called \"padding\" so that the sentence artificially becomes our desired length:\n",
      "# 08 - MLPs and Residual propagation\n",
      "\n",
      "So far, we have been using Logistic Regression for all our classification needs. Logistic regression is very similar to linear regression, except for that $\\sigma(z)$ in the end - it is essentially a linear projection and a choice between the \"positive\" and the \"negative\" sides of the projection surface.\n",
      "\n",
      "Also, we have seen that we can choose to project our data $X$ into an intermediate representation $z$ so that $z$ has more than one dimension. We can use that for multi-class classification.\n",
      "\n",
      "Now, we are going to view the effects of mapping the intermediate projection $z$ to another intermediate projection (let's call it $z_2$). As we will see, increasing the dimensionality of each representation $z_i$ and the number of intermediate projections has the effect of creating intermediate regions in which we can apply linear transformations separately. If you want a theoretical reference for such, refer to: [Thiago Serra, Christian Tjandraatmadja, Srikumar Ramalingam Proceedings of the 35th International Conference on Machine Learning, PMLR 80:4558-4566, 2018.](https://proceedings.mlr.press/v80/serra18b/serra18b.pdf).\n",
      "\n",
      "In the examples shown here, we will start with the use of Linear Regression to show some limitations of this approach. These animated examples work like this:\n",
      "\n",
      "1. We define a random dataset $X$\n",
      "1. We define a target $y$ by applying some function over $X$\n",
      "1. We initialize a prediction model with an identity function (that is, our weights are such that )\n",
      "1. We train the prediction model to predict $\\hat{y} = f(X)$ using gradient descent, and store $\\hat{y}_t$ for each iteration $t$\n",
      "1. We make an animation of all $\\hat{y}_t$ so we can see what happens to our predictions.\n",
      "\n",
      "\n",
      "## Dealing with large datasets\n",
      "\n",
      "Some datasets don't fit into our RAM memory, or into our GPU memory. To deal with such datasets, we need to progressively retrieve items and use them to perform small optimization steps. These small optimization steps are called *minibatches*. A minibatch typically has 16 to 128 items, but this can absolutely change according to your problem.\n",
      "\n",
      "PyTorch has an interface to make minibatches. It uses a combination of two types of objects: `DataSet` and `DataLoader`. A `DataSet` has the responsibility of retrieving one item from a set. A `DataLoader` combines the items returned from `DataSet` into batches. See the demo below:\n",
      "## Conclusion\n",
      "\n",
      "Althoug [our reference](https://proceedings.mlr.press/v80/serra18b/serra18b.pdf) states that there is a theoretical upper bound for the number of regions created by subsequent ReLU-separated projections, it is still challenging to find what are the optimal regions and corresponding projections for a particular dataset.\n",
      "\n",
      "Our toolset for such is:\n",
      "\n",
      "1. We can use simple linear regressions or logistic regressions to find a baseline for our system.\n",
      "1. We can use the MLP topology to create potential regions in our dataset. More layers, and more neurons per layer, increase the *expressivity* of the network, that is, the number of linear regions it can model.\n",
      "1. Adding a residual connection helps propagating gradients to the earlier layers of the MLP, which favors using the whole potential of the network.\n",
      "1. Normalization layers help leading to a more numerically stable fit.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=== FIM DO CONTEXTO ===\n",
      "Com base nas informações fornecidas, um tokenizador é um componente que converte texto em tokens.  Um token é uma parte do texto que carrega informações, podendo ser uma palavra, um n-grama ou até mesmo um sub-palavra (como em estratégias de tokenização sub-palavra).  O tokenizador pode realizar etapas como corte (truncating) e preenchimento (padding) para garantir que todas as sequências de entrada tenham o mesmo comprimento, necessário para processamento em modelos que utilizam tensores (como PyTorch).  Exemplos de tokenizadores incluem o Sentence Piece Tokenizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def rag_answer(question: str, notebook_paths: list, k) -> str:\n",
    "    # Carrega todas as células de todos os notebooks\n",
    "    all_cells = []\n",
    "    for path in notebook_paths:\n",
    "        all_cells.extend(load_notebook_cells(path))\n",
    "\n",
    "    markdown_cells = [c for c in all_cells if c['cell_type'] == 'markdown']\n",
    "    top = semantic_search(markdown_cells, question, k=k)\n",
    "\n",
    "    retrieved_text = '\\n'.join(''.join(c['source']) for c, _ in top)\n",
    "    print(\"=== CONTEXTO ===\")\n",
    "    print(retrieved_text)\n",
    "    print(\"=== FIM DO CONTEXTO ===\")\n",
    "    prompt = (\n",
    "        \"Use apenas as informações abaixo para responder à pergunta.\" +\n",
    "        \"\\n\\nInformações recuperadas:\\n\" + retrieved_text +\n",
    "        \"\\n\\nPergunta: \" + question\n",
    "    )\n",
    "    return llm_query(prompt, domain='NLP')\n",
    "\n",
    "# Exemplo de RAG\n",
    "\n",
    "notebook_paths = [os.path.join('../deep_learning', f) for f in os.listdir('../deep_learning') if f.endswith('.ipynb')]\n",
    "all_cells = []\n",
    "for path in notebook_paths:\n",
    "    all_cells.extend(load_notebook_cells(path))\n",
    "N = len(all_cells)\n",
    "print(N)\n",
    "question = \"O que é um tokenizador?\"\n",
    "answer = rag_answer(question, notebook_paths, N)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9114ccad",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "\n",
    "If you have reached this far, let's start optimizing our systems.\n",
    "\n",
    "To do so:\n",
    "\n",
    "1. Identify which step of your processing pipeline takes the longer\n",
    "1. Study if there are techniques or data structures that can make this specific step faster\n",
    "1. If possible, implement the optimization and test the results.\n",
    "1. Iterate until you cannot optimize anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02213387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo original: 55.83s\n",
      "Tempo otimizado: 0.10s\n",
      "Score: 0.8230\n",
      "Texto: ## Making a pipeline with PyTorch\n",
      "Score: 0.8093\n",
      "Texto: ## \n",
      "Score: 0.8045\n",
      "Texto: ## Visualizing embeddings\n",
      "Score: 0.7939\n",
      "Texto: ### Evaluating the model\n",
      "\n",
      "We can convert our \n",
      "Score: 0.7762\n",
      "Texto: # Modelling sequences\n",
      "Score: 0.7695\n",
      "Texto: ## Exercise\n",
      "\n",
      "Our usual way to approach classification is to do something in the lines of:\n",
      "Score: 0.7602\n",
      "Texto: ### Question 1: What is the underlying premise of the Bag-of-Words classifier, that is, why does BoW allow to classify these texts?\n",
      "Score: 0.7581\n",
      "Texto: \n",
      "\n",
      "```mermaid\n",
      "graph LR;\n",
      "    subgraph Inputs;\n",
      "    INPUT[\"[CLS]\n",
      "        remove\n",
      "        some\n",
      "        parts\n",
      "        [MASK]\n",
      "        a\n",
      "        sentence\"];\n",
      "    end;\n",
      "    INPUT --> BERT[\"BERT\"];\n",
      "    subgraph Ou\n",
      "Score: 0.7574\n",
      "Texto:     \n",
      "There are many details in this implementation, so I made a [video exploring them all](https://youtu.be/FXtGq_TYLzM).\n",
      "Score: 0.7505\n",
      "Texto: ### Example code to test the trained tokenizer:\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# --- 1) Profiling da implementação original ---\n",
    "start = time.time()\n",
    "_ = semantic_search(cells, \"minha pergunta aqui\", k=10)\n",
    "print(f\"Tempo original: {time.time() - start:.2f}s\")\n",
    "\n",
    "\n",
    "# --- 2) Pré-compute todos os embeddings de célula UMA ÚNICA VEZ ---\n",
    "# (supondo que `cells` já está carregado)\n",
    "cell_texts = [''.join(c['source']) for c in cells if c['cell_type'] == 'markdown']\n",
    "\n",
    "# Cria um array (n_células × dimensão do embedding)\n",
    "cell_embeddings = np.stack([embed_text(txt) for txt in cell_texts])\n",
    "\n",
    "\n",
    "# --- 3) Função de busca semântica VETORIZADA ---\n",
    "def semantic_search_fast(cell_texts: list[str],\n",
    "                         cell_embeddings: np.ndarray,\n",
    "                         query: str,\n",
    "                         k: int = 5) -> list[tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Retorna os top-k pares (texto_da_célula, score) mais similares.\n",
    "    Usa apenas NumPy + sklearn para calcular cosseno de forma vetorizada.\n",
    "    \"\"\"\n",
    "    # embedding da query\n",
    "    q_emb = embed_text(query).reshape(1, -1)\n",
    "\n",
    "    # cálculo de similaridade em batch\n",
    "    scores = cosine_similarity(cell_embeddings, q_emb).flatten()\n",
    "\n",
    "    # seleciona índices dos k maiores valores\n",
    "    top_idxs = np.argsort(scores)[-k:][::-1]\n",
    "\n",
    "    return [(cell_texts[i], float(scores[i])) for i in top_idxs]\n",
    "\n",
    "\n",
    "# --- 4) Profiling da versão otimizada ---\n",
    "start = time.time()\n",
    "topk = semantic_search_fast(cell_texts, cell_embeddings, \"minha pergunta aqui\", k=10)\n",
    "print(f\"Tempo otimizado: {time.time() - start:.2f}s\")\n",
    "\n",
    "# Exibe os resultados\n",
    "for txt, sc in topk:\n",
    "    print(f\"Score: {sc:.4f}\\nTexto: {txt[:200]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4ef498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
