{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5 - Scores\n",
    "\n",
    "## Recall (Sensibilidade)\n",
    "\n",
    "O Recall mede a proporção de casos positivos reais que foram corretamente identificados.\n",
    "\n",
    "```python\n",
    "Recall = TP / (TP + FN)\n",
    "```\n",
    "\n",
    "Onde:\n",
    "\n",
    "- TP = True Positives (Verdadeiros Positivos)\n",
    "- FN = False Negatives (Falsos Negativos)\n",
    "\n",
    "## Precision (Precisão)\n",
    "\n",
    "A Precision mede a proporção de identificações positivas que foram realmente corretas.\n",
    "\n",
    "```python\n",
    "Precision = TP / (TP + FP)\n",
    "```\n",
    "\n",
    "Onde:\n",
    "\n",
    "- TP = True Positives (Verdadeiros Positivos)\n",
    "- FP = False Positives (Falsos Positivos)\n",
    "\n",
    "## F1-Score\n",
    "\n",
    "O F1-Score é a média harmônica entre Precision e Recall, fornecendo um único valor que equilibra ambas as métricas.\n",
    "\n",
    "```python\n",
    "F1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "```\n",
    "\n",
    "## Balanced Accuracy Score\n",
    "\n",
    "O Balanced Accuracy Score é particularmente útil para conjuntos de dados desbalanceados, calculando a média entre a sensibilidade e a especificidade.\n",
    "\n",
    "```python\n",
    "Balanced Accuracy = (Sensitivity + Specificity) / 2\n",
    "Onde:\n",
    "Sensitivity = Recall = TP / (TP + FN)\n",
    "Specificity = TN / (TN + FP)\n",
    "```\n",
    "\n",
    "Onde:\n",
    "\n",
    "- TN = True Negatives (Verdadeiros Negativos)\n",
    "- FP = False Positives (Falsos Positivos)\n",
    "\n",
    "## Quando usar cada métrica?\n",
    "\n",
    "- **Recall:** Quando falsos negativos são mais custosos que falsos positivos (ex: diagnóstico de doenças)\n",
    "- **Precision:** Quando falsos positivos são mais custosos que falsos negativos (ex: spam detection)\n",
    "- **F1-Score:** Quando você precisa de um equilíbrio entre precision e recall\n",
    "- **Balanced Accuracy:** Quando suas classes são desbalanceadas e você quer dar igual importância para ambas\n",
    "\n",
    "Exemplo:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "Saida:\n",
    "\n",
    "precision    recall  f1-score   support\n",
    "\n",
    "      comedy       0.80      0.63      0.71       874\n",
    "       drama       0.77      0.89      0.82      1195\n",
    "\n",
    "    accuracy                           0.78      2069\n",
    "   macro avg       0.79      0.76      0.76      2069\n",
    "weighted avg       0.78      0.78      0.77      2069\n",
    "\n",
    "# Matriz de confusão\n",
    "\n",
    "O elemento c nas coordenadas (i, j) indica o número de vezes que um item cuja classe verdadeira é i, e a classe predita é j.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix,  ConfusionMatrixDisplay\n",
    "\n",
    "c = confusion_matrix(y_test, y_pred)\n",
    "print(c)\n",
    "_ = ConfusionMatrixDisplay(c, display_labels=['comedy', 'drama']).plot()\n",
    "```\n",
    "\n",
    "Saída:\n",
    "\n",
    "[[ 549  325]\n",
    "[ 133 1062]]\n",
    "\n",
    "![image.png](Notebook%205%20-%20Scores%201b1f1dbc878e80b1880de58d54865b72/image.png)\n",
    "\n",
    "Para mostrar as proporções de cada classificação, utilizar o parâmetro `normalize=’true’`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
