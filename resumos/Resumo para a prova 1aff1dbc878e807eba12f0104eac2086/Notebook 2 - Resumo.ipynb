{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2 - Document Frequency\n",
    "\n",
    "## **Document Frequency**\n",
    "\n",
    "- Número de **documentos** em que uma palavra aparece\n",
    "- Não é a mesma coisa que contar quantas vezes uma palavra aparece\n",
    "    - Se uma palavra aparece em 2 documentos, sua DF deve ser 2, mas se aparecer 2 vezes em 1 documento sua DF é 1\n",
    "\n",
    "## CountVectorizer - Sklearn\n",
    "\n",
    "- Cria uma matriz X de dimensões N x V, onde N é o número de documentos da coleção e V é o tamanho do vocabulário\n",
    "- O elemento x(n,v) pode ser:\n",
    "    - 1 → caso a palavra v aparecer no documento n\n",
    "    - 0 → caso a palavra v não aparecer no documento n\n",
    "- Cria uma propriedade `vocabulary_` contendo um dicionário que mapeia as palavras para seus respectivos índices\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "X = vectorizer.fit_transform(df_filt['description'])\n",
    "\n",
    "# Does the word 'the' appear in document 3?\n",
    "#X[3, vectorizer.vocabulary_['the']]\n",
    "\n",
    "# Continue with your solution\n",
    "doc_freq_matrix = X.mean(axis=0)\n",
    "doc_freq = {\n",
    "    word: doc_freq_matrix[0, vectorizer.vocabulary_[word]]\n",
    "    for word in vectorizer.vocabulary_\n",
    "}\n",
    "```\n",
    "\n",
    "## Lematização e Stemming\n",
    "\n",
    "São técnicas de normalização de texto usadas para reduzir palavras à sua forma base:\n",
    "\n",
    "- **Stemming:**\n",
    "    - Remove prefixos e sufixos das palavras de forma mecânica\n",
    "    - Mais rápido e simples, mas menos preciso\n",
    "    - Exemplo: \"correndo\" → \"corr\"\n",
    "- **Lematização:**\n",
    "    - Converte palavras para sua forma base (lema) usando análise morfológica\n",
    "    - Mais preciso, mas computacionalmente mais custoso\n",
    "    - Considera o contexto e significado da palavra\n",
    "    - Exemplo: \"correndo\" → \"correr\"\n",
    "- **Aplicação no Contexto:**\n",
    "    - Útil para reduzir a dimensionalidade do vocabulário\n",
    "    - Ajuda a melhorar a eficiência do CountVectorizer\n",
    "    - Agrupa variações da mesma palavra, melhorando o cálculo da frequência dos documentos\n",
    "\n",
    "## LDA (Latent Dirichlet Allocation)\n",
    "\n",
    "LDA é um modelo probabilístico generativo usado para descobrir tópicos latentes em uma coleção de documentos.\n",
    "\n",
    "- **Diferenças na Matriz:**\n",
    "    - No CountVectorizer: matriz binária N x V (documentos x vocabulário)\n",
    "    - No LDA: duas matrizes principais são geradas:\n",
    "        - Matriz documento-tópico (N x K): distribuição de tópicos para cada documento\n",
    "        - Matriz tópico-palavra (K x V): distribuição de palavras para cada tópico\n",
    "- **Características do LDA:**\n",
    "    - Cada documento é uma mistura de tópicos\n",
    "    - Cada tópico é uma distribuição sobre o vocabulário\n",
    "    - O número K de tópicos é definido previamente\n",
    "- **Vantagens:**\n",
    "    - Permite descoberta de estruturas temáticas latentes\n",
    "    - Reduz dimensionalidade dos dados\n",
    "    - Facilita interpretação semântica dos documentos\n",
    "\n",
    "Exemplo de implementação do LDA usando sklearn:\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Exemplo de documentos\n",
    "documentos = [\n",
    "    \"machine learning is fascinating and powerful\",\n",
    "    \"deep learning networks process data efficiently\",\n",
    "    \"neural networks learn patterns automatically\",\n",
    "    \"artificial intelligence transforms industries\"\n",
    "]\n",
    "\n",
    "# Criar o CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(documentos)\n",
    "\n",
    "# Configurar e treinar o modelo LDA\n",
    "n_topics = 2\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics,\n",
    "    random_state=42,\n",
    "    learning_method='batch'\n",
    ")\n",
    "lda_output = lda.fit_transform(X)\n",
    "\n",
    "# Visualizar os tópicos principais\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_words = [feature_names[i] for i in topic.argsort()[:-5:-1]]\n",
    "    print(f\"Tópico {topic_idx + 1}: {', '.join(top_words)}\")\n",
    "\n",
    "# Visualizar a distribuição de tópicos para cada documento\n",
    "for doc_idx, doc_topics in enumerate(lda_output):\n",
    "    print(f\"\\nDocumento {doc_idx + 1}:\")\n",
    "    for topic_idx, topic_prob in enumerate(doc_topics):\n",
    "        print(f\"Tópico {topic_idx + 1}: {topic_prob:.2f}\")\n",
    "```\n",
    "\n",
    "O código acima gera dois tipos principais de resultados:\n",
    "\n",
    "- **1. Palavras principais por tópico:**\n",
    "    - Para cada tópico, mostra as 4 palavras mais relevantes\n",
    "    - Exemplo: \"Tópico 1: learning, networks, neural, deep\"\n",
    "- **2. Distribuição de tópicos por documento:**\n",
    "    - Para cada documento, mostra a probabilidade dele pertencer a cada tópico\n",
    "    - Exemplo: \"Documento 1: Tópico 1 (0.60), Tópico 2 (0.40)\"\n",
    "    - Quanto maior o número, mais aquele documento está relacionado ao tópico\n",
    "    \n",
    "\n",
    "Em outras palavras, o LDA:\n",
    "\n",
    "- Agrupa palavras similares em tópicos\n",
    "- Indica quanto cada documento \"fala sobre\" cada tópico\n",
    "- Ajuda a entender os principais assuntos presentes nos textos"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
