{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4 - Classifiers\n",
    "\n",
    "# Bag-Of-Words\n",
    "\n",
    "O modelo Bag of Words (BoW) √© uma t√©cnica de processamento de linguagem natural que transforma texto em representa√ß√µes num√©ricas. Ele funciona criando um \"saco de palavras\" onde:\n",
    "\n",
    "- Cada palavra √∫nica do texto se torna uma caracter√≠stica (feature)\n",
    "- A ordem das palavras √© ignorada\n",
    "- A frequ√™ncia de cada palavra √© contada\n",
    "\n",
    "Exemplo pr√°tico de classifica√ß√£o de livros:\n",
    "\n",
    "Considere dois textos de sinopses:\n",
    "\n",
    "> Com√©dia: \"Uma hist√≥ria divertida e engra√ßada sobre uma fam√≠lia maluca em f√©rias\"\n",
    "Drama: \"Uma narrativa profunda e emocionante sobre perda e supera√ß√£o\"\n",
    "> \n",
    "\n",
    "O BoW criaria um vocabul√°rio como:\n",
    "\n",
    "```python\n",
    "vocabulario = {\n",
    "    'historia': 1, 'divertida': 2, 'engracada': 3, 'familia': 4, \n",
    "    'maluca': 5, 'ferias': 6, 'narrativa': 7, 'profunda': 8,\n",
    "    'emocionante': 9, 'perda': 10, 'superacao': 11\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "Cada texto seria representado como um vetor:\n",
    "\n",
    "```python\n",
    "comedia = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]  # presen√ßa das palavras\n",
    "drama =   [1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
    "\n",
    "```\n",
    "\n",
    "Um classificador poderia ent√£o **aprender padr√µes nestes vetores para categorizar novos textos em \"com√©dia\" ou \"drama\" baseado nas palavras presentes.**\n",
    "\n",
    "<aside>\n",
    "üí° Esta representa√ß√£o, embora simples, √© efetiva para muitas tarefas de classifica√ß√£o de texto, especialmente quando combinada com t√©cnicas de normaliza√ß√£o e sele√ß√£o de caracter√≠sticas.\n",
    "\n",
    "</aside>\n",
    "\n",
    "# Distribui√ß√£o de Bernoulli\n",
    "\n",
    "A distribui√ß√£o de Bernoulli √© uma varia√ß√£o do modelo Bag-of-Words que considera apenas a presen√ßa (1) ou aus√™ncia (0) de palavras, ignorando suas frequ√™ncias.\n",
    "\n",
    "Principais caracter√≠sticas:\n",
    "\n",
    "- Cada palavra √© representada por um valor bin√°rio (0 ou 1)\n",
    "- N√£o importa quantas vezes a palavra aparece no texto\n",
    "- √ötil para textos curtos ou quando a frequ√™ncia n√£o √© relevante\n",
    "\n",
    "Exemplo usando o caso anterior:\n",
    "\n",
    "```python\n",
    "# Mesmo com m√∫ltiplas ocorr√™ncias, s√≥ marcamos presen√ßa (1) ou aus√™ncia (0)\n",
    "comedia = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]  # presen√ßa das palavras\n",
    "drama =   [1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Se \"divertida\" aparecesse 3 vezes, ainda seria representada como 1\n",
    "\n",
    "```\n",
    "\n",
    "<aside>\n",
    "üí° A distribui√ß√£o de Bernoulli pode ser mais eficiente computacionalmente e menos sens√≠vel a outliers, j√° que n√£o precisamos lidar com contagens.\n",
    "\n",
    "</aside>\n",
    "\n",
    "# Regress√£o Log√≠stica\n",
    "\n",
    "A Regress√£o Log√≠stica √© um algoritmo de classifica√ß√£o que estima a probabilidade de um evento pertencer a uma determinada classe.\n",
    "\n",
    "Principais caracter√≠sticas:\n",
    "\n",
    "- Modelo linear que usa uma fun√ß√£o sigm√≥ide para mapear valores para probabilidades entre 0 e 1\n",
    "- Eficiente para problemas de classifica√ß√£o bin√°ria\n",
    "- Fornece probabilidades interpret√°veis como output\n",
    "\n",
    "Exemplo para classifica√ß√£o de texto:\n",
    "\n",
    "```python\n",
    "# Usando vetores BoW como input para regress√£o log√≠stica\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# X cont√©m os vetores BoW, y cont√©m as classes (0: drama, 1: com√©dia)\n",
    "modelo = LogisticRegression()\n",
    "modelo.fit(X, y)\n",
    "\n",
    "# Obt√©m probabilidades para um novo texto\n",
    "probabilidades = modelo.predict_proba(novo_texto_vetorizado) # Probabilidades\n",
    "predict = modelo.predict(novo_texto_vetorizado) # Classifica√ß√£o\n",
    "```\n",
    "\n",
    "<aside>\n",
    "üí° A Regress√£o Log√≠stica √© particularmente √∫til quando precisamos n√£o s√≥ da classifica√ß√£o, mas tamb√©m da probabilidade/confian√ßa da predi√ß√£o.\n",
    "\n",
    "</aside>\n",
    "\n",
    "Aqui est√° um resumo das caracter√≠sticas da Regress√£o Log√≠stica:\n",
    "\n",
    "| **Caracter√≠stica** | **Descri√ß√£o** |\n",
    "| --- | --- |\n",
    "| Tipo de classifica√ß√£o | Bin√°ria (naturalmente) ou multiclasse (one-vs-rest) |\n",
    "| Output | Probabilidades entre 0 e 1 |\n",
    "| Complexidade | Baixa (modelo linear) |\n",
    "| Interpretabilidade | Alta (coeficientes indicam import√¢ncia das features) |\n",
    "| Requisitos de dados | Features num√©ricas, preferencialmente escaladas |\n",
    "| Uso em NLP | Muito comum com BoW ou TF-IDF como input |\n",
    "\n",
    "## Compara√ß√£o dos Modelos\n",
    "\n",
    "Aqui est√° uma an√°lise comparativa detalhada dos tr√™s modelos discutidos:\n",
    "\n",
    "| **Caracter√≠stica** | **Bag-of-Words** | **Distribui√ß√£o de Bernoulli** | **Regress√£o Log√≠stica** |\n",
    "| --- | --- | --- | --- |\n",
    "| Representa√ß√£o | Frequ√™ncia das palavras | Presen√ßa/aus√™ncia (bin√°rio) | Probabilidades (0-1) |\n",
    "| Complexidade | M√©dia | Baixa | Baixa |\n",
    "| Uso de mem√≥ria | Alto (armazena frequ√™ncias) | Baixo (apenas bin√°rio) | Baixo |\n",
    "| Melhor cen√°rio | Textos longos com frequ√™ncias importantes | Textos curtos, documentos bin√°rios | Classifica√ß√£o bin√°ria com features num√©ricas |\n",
    "| Limita√ß√µes | Ignora ordem das palavras | Perde informa√ß√£o de frequ√™ncia | Assume rela√ß√£o linear entre features |\n",
    "| Interpretabilidade | M√©dia | Alta | Alta |\n",
    "| Tipo de output | Vetores num√©ricos | Vetores bin√°rios | Probabilidades |\n",
    "\n",
    "<aside>\n",
    "üí° Cada modelo tem seus pontos fortes espec√≠ficos:\n",
    "- BoW √© excelente para an√°lise detalhada de frequ√™ncia de palavras\n",
    "- Bernoulli √© eficiente para classifica√ß√£o simples de textos curtos\n",
    "- Regress√£o Log√≠stica oferece probabilidades interpret√°veis para tomada de decis√£o\n",
    "\n",
    "</aside>\n",
    "\n",
    "# TFIDF\n",
    "\n",
    "- Term-Frequency-Inverse-Document-Frequency\n",
    "    - TF: frequ√™ncia de cada termo em cada documento (d√° uma ideia da import√¢ncia de cada termo em cada documento)\n",
    "    - DF: import√¢ncia de um termo para a cole√ß√£o inteira\n",
    "        - Um termo com um DF pequeno, tende a ser mais relevante em um documento\n",
    "\n",
    "## Resumo dos Conceitos\n",
    "\n",
    "Neste notebook, foram abordados importantes conceitos de classifica√ß√£o de texto e processamento de linguagem natural:\n",
    "\n",
    "- **Bag-of-Words (BoW):**\n",
    "    - T√©cnica que transforma texto em representa√ß√£o num√©rica\n",
    "    - Conta a frequ√™ncia de cada palavra no texto\n",
    "    - Ignora a ordem das palavras\n",
    "- **Distribui√ß√£o de Bernoulli:**\n",
    "    - Varia√ß√£o do BoW que considera apenas presen√ßa (1) ou aus√™ncia (0)\n",
    "    - Mais eficiente computacionalmente\n",
    "    - Ideal para textos curtos\n",
    "- **Regress√£o Log√≠stica:**\n",
    "    - Algoritmo de classifica√ß√£o que estima probabilidades\n",
    "    - Usa fun√ß√£o sigm√≥ide para mapear valores entre 0 e 1\n",
    "    - Alta interpretabilidade e baixa complexidade\n",
    "- **TFIDF (Term Frequency-Inverse Document Frequency):**\n",
    "    - Combina frequ√™ncia do termo no documento (TF)\n",
    "    - Considera a relev√¢ncia do termo na cole√ß√£o inteira (IDF)\n",
    "    - Permite identificar termos mais distintivos em documentos\n",
    "\n",
    "| Modelo                  | Melhor Cen√°rio de Uso                            | Exemplo de Aplica√ß√£o            |\n",
    "|-------------------------|--------------------------------------------------|---------------------------------|\n",
    "| **Bag-of-Words**        | - Textos longos                                 | - An√°lise de artigos cient√≠ficos|\n",
    "|                         | - Frequ√™ncia √© importante                        | - Categoriza√ß√£o de documentos longos|\n",
    "|                         | - An√°lise detalhada do conte√∫do                  | - An√°lise de reviews detalhados |\n",
    "| **Distribui√ß√£o de Bernoulli** | - Textos curtos                            | - Classifica√ß√£o de tweets       |\n",
    "|                         | - Classifica√ß√£o simples                          | - An√°lise de mensagens curtas   |\n",
    "|                         | - Recursos computacionais limitados              | - Filtro de spam                |\n",
    "| **Regress√£o Log√≠stica** | - Necessidade de probabilidades                  | - Previs√£o de sentimentos       |\n",
    "|                         | - Classifica√ß√£o bin√°ria                          | - Detec√ß√£o de fraude            |\n",
    "|                         | - Dados bem estruturados                         | - Classifica√ß√£o de clientes     |\n",
    "| **TF-IDF**              | - Identifica√ß√£o de termos relevantes             | - Sistemas de busca             |\n",
    "|                         | - An√°lise de cole√ß√µes de documentos              | - Recomenda√ß√£o de conte√∫do      |\n",
    "|                         | - Busca por termos distintivos                   | - An√°lise de relev√¢ncia de documentos|\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
